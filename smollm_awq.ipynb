{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import textwrap\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"HuggingFaceTB/SmolLM-135M\"\n",
    "quantized_model_name = \"TechxGenus/SmolLM-135M-Instruct-AWQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_map = {\"\": 0}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device_map)\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(\n",
    "    quantized_model_name, device_map=device_map\n",
    ")\n",
    "print(model)\n",
    "print(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_memory_usage(model):\n",
    "  \"\"\"Calculates the memory usage of a PyTorch model.\"\"\"\n",
    "  total_memory = 0\n",
    "  for param in model.parameters():\n",
    "    total_memory += param.element_size() * param.numel()\n",
    "  return total_memory\n",
    "\n",
    "# Calculate memory usage for the original model\n",
    "model_memory = get_model_memory_usage(model)\n",
    "print(f\"Original model memory usage: {model_memory / (1024**2):.2f} MB\")\n",
    "# Calculate memory usage for the quantized model\n",
    "quantized_model_memory = get_model_memory_usage(quantized_model)\n",
    "print(f\"Quantized model memory usage: {quantized_model_memory / (1024**2):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function returns the outputs from the model received, and inputs.\n",
    "def get_outputs(model, inputs, max_new_tokens=200):\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        repetition_penalty=1.1,\n",
    "        early_stopping=False, #Can stop before reach the max_length\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Input text\n",
    "input_text = \"Tell a short history of humanity with happy ending.\"\n",
    "# Example Output function\n",
    "def example_output_tokens(model, tokenizer, input_text):\n",
    "    input_sentences = tokenizer(input_text, return_tensors=\"pt\").to('cuda')\n",
    "    foundational_outputs_sentence = get_outputs(model, input_sentences, max_new_tokens=100)\n",
    "    return foundational_outputs_sentence\n",
    "\n",
    "def example_output_text(tokenizer, tokens):\n",
    "    return tokenizer.batch_decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "start = time.time()\n",
    "tokens = example_output_tokens(quantized_model, tokenizer, input_text)\n",
    "print(f\"Time taken to generate tokens: {time.time() - start}\")\n",
    "text = example_output_text(tokenizer, tokens)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beautify_text(text):\n",
    "    print(\"Generated Output:\\n\")\n",
    "    for i, sentence in enumerate(text, 1):\n",
    "        wrapped_sentence = textwrap.fill(sentence, width=80)\n",
    "        print(f\"Output {i}:\\n{wrapped_sentence}\\n\")\n",
    "\n",
    "print(beautify_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    model.model.layers[i].self_attn.q_proj = quantized_model.model.layers[i].self_attn.q_proj\n",
    "    model.model.layers[i].self_attn.k_proj = quantized_model.model.layers[i].self_attn.k_proj\n",
    "    model.model.layers[i].self_attn.v_proj = quantized_model.model.layers[i].self_attn.v_proj\n",
    "\n",
    "print(\"Hybrid model memory usage: \", get_model_memory_usage(model) / (1024**2))\n",
    "# run with torch autocast\n",
    "with torch.amp.autocast(device_type=\"cuda\"):\n",
    "    start = time.time()\n",
    "    output_tokens = example_output_tokens(model, tokenizer, input_text)\n",
    "    print(\"Time taken for hybrid model: \", time.time() - start)\n",
    "output_text = example_output_text(tokenizer, output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(beautify_text(output_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
