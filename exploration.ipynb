{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dotenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import textwrap\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"HuggingFaceTB/SmolLM-135M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_map = {\"\": 0}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device_map)\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, quantization_config=bnb_config, device_map=device_map\n",
    ")\n",
    "dequantized_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, quantization_config=bnb_config, device_map=device_map\n",
    ").dequantize()\n",
    "print(model)\n",
    "print(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function returns the outputs from the model received, and inputs.\n",
    "def get_outputs(model, inputs, max_new_tokens=200):\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        repetition_penalty=1.1,\n",
    "        early_stopping=False, #Can stop before reach the max_length\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Input text\n",
    "input_text = \"Tell a short history of humanity with happy ending.\"\n",
    "# Example Output function\n",
    "def example_output_tokens(model, tokenizer, input_text):\n",
    "    input_sentences = tokenizer(input_text, return_tensors=\"pt\").to('cuda')\n",
    "    foundational_outputs_sentence = get_outputs(model, input_sentences, max_new_tokens=100)\n",
    "    return foundational_outputs_sentence\n",
    "\n",
    "def example_output_text(tokenizer, tokens):\n",
    "    return tokenizer.batch_decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "start = time.time()\n",
    "tokens = example_output_tokens(model, tokenizer, input_text)\n",
    "print(f\"Time taken to generate tokens: {time.time() - start}\")\n",
    "text = example_output_text(tokenizer, tokens)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beautify_text(text):\n",
    "    print(\"Generated Output:\\n\")\n",
    "    for i, sentence in enumerate(text, 1):\n",
    "        wrapped_sentence = textwrap.fill(sentence, width=80)\n",
    "        print(f\"Output {i}:\\n{wrapped_sentence}\\n\")\n",
    "\n",
    "print(beautify_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_embeddings = model.model.embed_tokens.weight == quantized_model.model.embed_tokens.weight\n",
    "# check if tensor_1 is all bool values\n",
    "torch.all(compare_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_memory_usage(model):\n",
    "  \"\"\"Calculates the memory usage of a PyTorch model.\"\"\"\n",
    "  total_memory = 0\n",
    "  for param in model.parameters():\n",
    "    total_memory += param.element_size() * param.numel()\n",
    "  return total_memory\n",
    "\n",
    "# Calculate memory usage for the quantized model\n",
    "quantized_model_memory = get_model_memory_usage(quantized_model)\n",
    "print(f\"Quantized model memory usage: {quantized_model_memory / (1024**2):.2f} MB\")\n",
    "\n",
    "# Calculate memory usage for the original model\n",
    "original_model_memory = get_model_memory_usage(model)\n",
    "print(f\"Original model memory usage: {original_model_memory / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of attention heads in the model\n",
    "num_attention_heads = model.config.num_attention_heads\n",
    "print(f\"Number of attention heads: {num_attention_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.model.layers[0].self_attn.q_proj.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all tensors of model to not require gradients\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "print(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace embedding\n",
    "# model.model.embed_tokens.weight.data = dequantized_model.model.embed_tokens.weight.data\n",
    "model.model.embed_tokens.weight.data = quantized_model.model.embed_tokens.weight.data\n",
    "# replace first 30 att heads from quantized model to original model\n",
    "for i in range(2):\n",
    "    model.model.layers[i].self_attn.q_proj = quantized_model.model.layers[i].self_attn.q_proj\n",
    "    model.model.layers[i].self_attn.k_proj = quantized_model.model.layers[i].self_attn.k_proj\n",
    "    model.model.layers[i].self_attn.v_proj = quantized_model.model.layers[i].self_attn.v_proj\n",
    "\n",
    "print(\"Hybrid model memory usage: \", get_model_memory_usage(model) / (1024**2))\n",
    "# run with torch autocast\n",
    "with torch.amp.autocast(device_type=\"cuda\"):\n",
    "    start = time.time()\n",
    "    output_tokens = example_output_tokens(model, tokenizer, input_text)\n",
    "    print(\"Time taken for hybrid model: \", time.time() - start)\n",
    "output_text = example_output_text(tokenizer, output_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for att_head in model.model.layers:\n",
    "    print(att_head.self_attn.q_proj)\n",
    "    print(att_head.self_attn.k_proj)\n",
    "    print(att_head.self_attn.v_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(beautify_text(output_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
