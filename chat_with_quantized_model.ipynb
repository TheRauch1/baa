{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from baa import (\n",
    "    QuantizedLinearLayerWithActivation,\n",
    "    replace_linear_layer_with_activation,\n",
    "    register_linear_layer_forward_hook,\n",
    "    device_map,\n",
    "    get_hidden_states_input,\n",
    "    get_weights,\n",
    "    add_custom_name_to_linear_layers,\n",
    "    remove_all_hooks,\n",
    "    chat_with_model,\n",
    "    print_memory_usage,\n",
    "    AccuracyBenchmark,\n",
    ")\n",
    "from baa.singletons import hidden_states, names\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "model_name = \"HuggingFaceTB/SmolLM-135M\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device_map)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 576)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Output:\n",
      "\n",
      "Output 1:\n",
      "Hi there how are you?  I'm not sure if this is the right place to ask this\n",
      "question, but I'm not sure if it's the right place to ask this question.  I'm\n",
      "not sure if this is the right place to ask this question.  I'm not sure if this\n",
      "is the right place to ask this question.  I'm not sure if this is the right\n",
      "place to ask this question.  I'm not sure if this is the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat_with_model(model, tokenizer, \"Hi there how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "benchmark = AccuracyBenchmark(model, tokenizer, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 41/45 [00:29<00:02,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model accuracy: 0.47169398907103827\n",
      "exclude_list: []\n",
      "hidden_states is empty: True\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    add_custom_name_to_linear_layers(model)\n",
    "    # register_linear_layer_forward_hook(model, get_hidden_states_input)\n",
    "    print(\"Original model accuracy:\", benchmark.evaluate(sample_size=300))\n",
    "    layers = []\n",
    "    # add elemnt of names to string if element is not in string \"mlp\"\n",
    "    exclude_list = []\n",
    "    print(\"exclude_list:\", exclude_list)\n",
    "    print(\n",
    "        f\"hidden_states is empty: {not bool(hidden_states)}\"\n",
    "    )  # empty dicts resolve to False\n",
    "    replace_linear_layer_with_activation(\n",
    "        base_model=model,\n",
    "        quantizer_class=QuantizedLinearLayerWithActivation,\n",
    "        weight_bits=5,\n",
    "        activation_bits=16,\n",
    "        exclude_list=exclude_list,\n",
    "        quantized=True,\n",
    "    )\n",
    "    remove_all_hooks(model)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 576)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): QuantizedLinearLayerWithActivation()\n",
       "          (k_proj): QuantizedLinearLayerWithActivation()\n",
       "          (v_proj): QuantizedLinearLayerWithActivation()\n",
       "          (o_proj): QuantizedLinearLayerWithActivation()\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantizedLinearLayerWithActivation()\n",
       "          (up_proj): QuantizedLinearLayerWithActivation()\n",
       "          (down_proj): QuantizedLinearLayerWithActivation()\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): QuantizedLinearLayerWithActivation()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 22/26 [00:16<00:02,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model accuracy: 0.44423047296256357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    print(\"Quantized model accuracy:\", benchmark.evaluate(sample_size=200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Output:\n",
      "\n",
      "Output 1:\n",
      "Hi there how are you? I am 18 years old and I am 18 years old I am 18 years old\n",
      "I am 18 years old I am 18 years old I am 18 years old I am 18 years old I am 18\n",
      "years old I am 18 years old I am 18 years old I am 18 years old I am 18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat_with_model(model, tokenizer, \"Hi there how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15, device='mps:0', dtype=torch.int8)\n",
      "tensor(-16, device='mps:0', dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "print(model.model.layers[0].self_attn.k_proj.weight.max())\n",
    "print(model.model.layers[0].self_attn.k_proj.weight.min())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
