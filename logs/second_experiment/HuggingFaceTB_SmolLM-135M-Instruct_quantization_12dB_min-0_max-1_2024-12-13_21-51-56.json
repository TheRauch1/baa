{
    "model_name": "HuggingFaceTB/SmolLM-135M-Instruct",
    "layerwise_quantization_info": {
        "model.layers.0.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.223129272460938
        },
        "model.layers.0.self_attn.k_proj": {
            "bit_width": 3,
            "error": 14.537639617919922
        },
        "model.layers.0.self_attn.v_proj": {
            "bit_width": 4,
            "error": 15.866477966308594
        },
        "model.layers.0.self_attn.o_proj": {
            "bit_width": 5,
            "error": 15.08546257019043
        },
        "model.layers.0.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.52315330505371
        },
        "model.layers.0.mlp.up_proj": {
            "bit_width": 5,
            "error": 18.027740478515625
        },
        "model.layers.0.mlp.down_proj": {
            "bit_width": 4,
            "error": 18.302528381347656
        },
        "model.layers.1.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.901411056518555
        },
        "model.layers.1.self_attn.k_proj": {
            "bit_width": 4,
            "error": 16.367019653320312
        },
        "model.layers.1.self_attn.v_proj": {
            "bit_width": 4,
            "error": 18.090579986572266
        },
        "model.layers.1.self_attn.o_proj": {
            "bit_width": 4,
            "error": 15.596183776855469
        },
        "model.layers.1.mlp.gate_proj": {
            "bit_width": 4,
            "error": 17.240455627441406
        },
        "model.layers.1.mlp.up_proj": {
            "bit_width": 4,
            "error": 17.334186553955078
        },
        "model.layers.1.mlp.down_proj": {
            "bit_width": 4,
            "error": 14.85596752166748
        },
        "model.layers.2.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.094022750854492
        },
        "model.layers.2.self_attn.k_proj": {
            "bit_width": 4,
            "error": 14.45555305480957
        },
        "model.layers.2.self_attn.v_proj": {
            "bit_width": 4,
            "error": 17.961952209472656
        },
        "model.layers.2.self_attn.o_proj": {
            "bit_width": 4,
            "error": 15.827644348144531
        },
        "model.layers.2.mlp.gate_proj": {
            "bit_width": 4,
            "error": 17.568580627441406
        },
        "model.layers.2.mlp.up_proj": {
            "bit_width": 4,
            "error": 15.522712707519531
        },
        "model.layers.2.mlp.down_proj": {
            "bit_width": 5,
            "error": 12.058521270751953
        },
        "model.layers.3.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.20682144165039
        },
        "model.layers.3.self_attn.k_proj": {
            "bit_width": 4,
            "error": 17.05795669555664
        },
        "model.layers.3.self_attn.v_proj": {
            "bit_width": 3,
            "error": 12.283727645874023
        },
        "model.layers.3.self_attn.o_proj": {
            "bit_width": 4,
            "error": 17.354236602783203
        },
        "model.layers.3.mlp.gate_proj": {
            "bit_width": 4,
            "error": 17.7208309173584
        },
        "model.layers.3.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.716266632080078
        },
        "model.layers.3.mlp.down_proj": {
            "bit_width": 4,
            "error": 13.76675033569336
        },
        "model.layers.4.self_attn.q_proj": {
            "bit_width": 4,
            "error": 15.816354751586914
        },
        "model.layers.4.self_attn.k_proj": {
            "bit_width": 4,
            "error": 18.226282119750977
        },
        "model.layers.4.self_attn.v_proj": {
            "bit_width": 3,
            "error": 12.742809295654297
        },
        "model.layers.4.self_attn.o_proj": {
            "bit_width": 4,
            "error": 17.320716857910156
        },
        "model.layers.4.mlp.gate_proj": {
            "bit_width": 4,
            "error": 18.095962524414062
        },
        "model.layers.4.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.570087432861328
        },
        "model.layers.4.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.7736873626709
        },
        "model.layers.5.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.667800903320312
        },
        "model.layers.5.self_attn.k_proj": {
            "bit_width": 4,
            "error": 17.852094650268555
        },
        "model.layers.5.self_attn.v_proj": {
            "bit_width": 3,
            "error": 13.034534454345703
        },
        "model.layers.5.self_attn.o_proj": {
            "bit_width": 4,
            "error": 17.49818229675293
        },
        "model.layers.5.mlp.gate_proj": {
            "bit_width": 3,
            "error": 12.185670852661133
        },
        "model.layers.5.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.736751556396484
        },
        "model.layers.5.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.088665008544922
        },
        "model.layers.6.self_attn.q_proj": {
            "bit_width": 4,
            "error": 17.181114196777344
        },
        "model.layers.6.self_attn.k_proj": {
            "bit_width": 4,
            "error": 17.576614379882812
        },
        "model.layers.6.self_attn.v_proj": {
            "bit_width": 3,
            "error": 12.028132438659668
        },
        "model.layers.6.self_attn.o_proj": {
            "bit_width": 4,
            "error": 18.156047821044922
        },
        "model.layers.6.mlp.gate_proj": {
            "bit_width": 4,
            "error": 18.28890609741211
        },
        "model.layers.6.mlp.up_proj": {
            "bit_width": 4,
            "error": 17.117233276367188
        },
        "model.layers.6.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.845592498779297
        },
        "model.layers.7.self_attn.q_proj": {
            "bit_width": 4,
            "error": 15.40757942199707
        },
        "model.layers.7.self_attn.k_proj": {
            "bit_width": 4,
            "error": 17.25540542602539
        },
        "model.layers.7.self_attn.v_proj": {
            "bit_width": 3,
            "error": 12.610824584960938
        },
        "model.layers.7.self_attn.o_proj": {
            "bit_width": 4,
            "error": 17.40652084350586
        },
        "model.layers.7.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.970857620239258
        },
        "model.layers.7.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.51020050048828
        },
        "model.layers.7.mlp.down_proj": {
            "bit_width": 4,
            "error": 16.956085205078125
        },
        "model.layers.8.self_attn.q_proj": {
            "bit_width": 4,
            "error": 18.13596534729004
        },
        "model.layers.8.self_attn.k_proj": {
            "bit_width": 3,
            "error": 13.375872611999512
        },
        "model.layers.8.self_attn.v_proj": {
            "bit_width": 4,
            "error": 18.160093307495117
        },
        "model.layers.8.self_attn.o_proj": {
            "bit_width": 4,
            "error": 17.71462631225586
        },
        "model.layers.8.mlp.gate_proj": {
            "bit_width": 4,
            "error": 18.036449432373047
        },
        "model.layers.8.mlp.up_proj": {
            "bit_width": 4,
            "error": 15.222014427185059
        },
        "model.layers.8.mlp.down_proj": {
            "bit_width": 4,
            "error": 16.96816635131836
        },
        "model.layers.9.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.659671783447266
        },
        "model.layers.9.self_attn.k_proj": {
            "bit_width": 4,
            "error": 16.983694076538086
        },
        "model.layers.9.self_attn.v_proj": {
            "bit_width": 4,
            "error": 18.266141891479492
        },
        "model.layers.9.self_attn.o_proj": {
            "bit_width": 4,
            "error": 15.61296558380127
        },
        "model.layers.9.mlp.gate_proj": {
            "bit_width": 4,
            "error": 17.969839096069336
        },
        "model.layers.9.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.571428298950195
        },
        "model.layers.9.mlp.down_proj": {
            "bit_width": 4,
            "error": 16.97623062133789
        },
        "model.layers.10.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.942901611328125
        },
        "model.layers.10.self_attn.k_proj": {
            "bit_width": 4,
            "error": 18.184938430786133
        },
        "model.layers.10.self_attn.v_proj": {
            "bit_width": 3,
            "error": 12.356386184692383
        },
        "model.layers.10.self_attn.o_proj": {
            "bit_width": 4,
            "error": 18.631628036499023
        },
        "model.layers.10.mlp.gate_proj": {
            "bit_width": 3,
            "error": 12.047761917114258
        },
        "model.layers.10.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.521814346313477
        },
        "model.layers.10.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.630680084228516
        },
        "model.layers.11.self_attn.q_proj": {
            "bit_width": 4,
            "error": 15.356979370117188
        },
        "model.layers.11.self_attn.k_proj": {
            "bit_width": 3,
            "error": 12.501068115234375
        },
        "model.layers.11.self_attn.v_proj": {
            "bit_width": 3,
            "error": 12.414345741271973
        },
        "model.layers.11.self_attn.o_proj": {
            "bit_width": 4,
            "error": 18.230510711669922
        },
        "model.layers.11.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.099058151245117
        },
        "model.layers.11.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.026973724365234
        },
        "model.layers.11.mlp.down_proj": {
            "bit_width": 4,
            "error": 12.888402938842773
        },
        "model.layers.12.self_attn.q_proj": {
            "bit_width": 4,
            "error": 14.5271577835083
        },
        "model.layers.12.self_attn.k_proj": {
            "bit_width": 4,
            "error": 16.425745010375977
        },
        "model.layers.12.self_attn.v_proj": {
            "bit_width": 3,
            "error": 12.147287368774414
        },
        "model.layers.12.self_attn.o_proj": {
            "bit_width": 4,
            "error": 16.602603912353516
        },
        "model.layers.12.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.683263778686523
        },
        "model.layers.12.mlp.up_proj": {
            "bit_width": 4,
            "error": 15.39509391784668
        },
        "model.layers.12.mlp.down_proj": {
            "bit_width": 5,
            "error": 14.306114196777344
        },
        "model.layers.13.self_attn.q_proj": {
            "bit_width": 4,
            "error": 13.344926834106445
        },
        "model.layers.13.self_attn.k_proj": {
            "bit_width": 4,
            "error": 14.695490837097168
        },
        "model.layers.13.self_attn.v_proj": {
            "bit_width": 3,
            "error": 13.005932807922363
        },
        "model.layers.13.self_attn.o_proj": {
            "bit_width": 3,
            "error": 12.128938674926758
        },
        "model.layers.13.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.413150787353516
        },
        "model.layers.13.mlp.up_proj": {
            "bit_width": 4,
            "error": 15.89883041381836
        },
        "model.layers.13.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.267471313476562
        },
        "model.layers.14.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.98076629638672
        },
        "model.layers.14.self_attn.k_proj": {
            "bit_width": 4,
            "error": 18.601669311523438
        },
        "model.layers.14.self_attn.v_proj": {
            "bit_width": 4,
            "error": 17.293109893798828
        },
        "model.layers.14.self_attn.o_proj": {
            "bit_width": 4,
            "error": 17.582387924194336
        },
        "model.layers.14.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.59357452392578
        },
        "model.layers.14.mlp.up_proj": {
            "bit_width": 4,
            "error": 13.938236236572266
        },
        "model.layers.14.mlp.down_proj": {
            "bit_width": 4,
            "error": 16.847593307495117
        },
        "model.layers.15.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.02961540222168
        },
        "model.layers.15.self_attn.k_proj": {
            "bit_width": 4,
            "error": 13.680255889892578
        },
        "model.layers.15.self_attn.v_proj": {
            "bit_width": 3,
            "error": 13.012552261352539
        },
        "model.layers.15.self_attn.o_proj": {
            "bit_width": 4,
            "error": 15.752532958984375
        },
        "model.layers.15.mlp.gate_proj": {
            "bit_width": 4,
            "error": 15.802817344665527
        },
        "model.layers.15.mlp.up_proj": {
            "bit_width": 4,
            "error": 15.85947036743164
        },
        "model.layers.15.mlp.down_proj": {
            "bit_width": 4,
            "error": 16.857250213623047
        },
        "model.layers.16.self_attn.q_proj": {
            "bit_width": 4,
            "error": 13.643924713134766
        },
        "model.layers.16.self_attn.k_proj": {
            "bit_width": 4,
            "error": 14.151567459106445
        },
        "model.layers.16.self_attn.v_proj": {
            "bit_width": 3,
            "error": 12.656903266906738
        },
        "model.layers.16.self_attn.o_proj": {
            "bit_width": 4,
            "error": 18.498985290527344
        },
        "model.layers.16.mlp.gate_proj": {
            "bit_width": 4,
            "error": 15.845300674438477
        },
        "model.layers.16.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.179101943969727
        },
        "model.layers.16.mlp.down_proj": {
            "bit_width": 4,
            "error": 16.740196228027344
        },
        "model.layers.17.self_attn.q_proj": {
            "bit_width": 4,
            "error": 14.742301940917969
        },
        "model.layers.17.self_attn.k_proj": {
            "bit_width": 4,
            "error": 14.304288864135742
        },
        "model.layers.17.self_attn.v_proj": {
            "bit_width": 3,
            "error": 12.941978454589844
        },
        "model.layers.17.self_attn.o_proj": {
            "bit_width": 4,
            "error": 16.541770935058594
        },
        "model.layers.17.mlp.gate_proj": {
            "bit_width": 4,
            "error": 15.93556022644043
        },
        "model.layers.17.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.07438850402832
        },
        "model.layers.17.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.14446258544922
        },
        "model.layers.18.self_attn.q_proj": {
            "bit_width": 4,
            "error": 13.488472938537598
        },
        "model.layers.18.self_attn.k_proj": {
            "bit_width": 4,
            "error": 14.368446350097656
        },
        "model.layers.18.self_attn.v_proj": {
            "bit_width": 4,
            "error": 17.715656280517578
        },
        "model.layers.18.self_attn.o_proj": {
            "bit_width": 4,
            "error": 18.303529739379883
        },
        "model.layers.18.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.089582443237305
        },
        "model.layers.18.mlp.up_proj": {
            "bit_width": 4,
            "error": 15.45528507232666
        },
        "model.layers.18.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.12293815612793
        },
        "model.layers.19.self_attn.q_proj": {
            "bit_width": 4,
            "error": 14.655165672302246
        },
        "model.layers.19.self_attn.k_proj": {
            "bit_width": 4,
            "error": 14.837969779968262
        },
        "model.layers.19.self_attn.v_proj": {
            "bit_width": 4,
            "error": 17.26232147216797
        },
        "model.layers.19.self_attn.o_proj": {
            "bit_width": 4,
            "error": 16.779537200927734
        },
        "model.layers.19.mlp.gate_proj": {
            "bit_width": 4,
            "error": 15.920191764831543
        },
        "model.layers.19.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.90915298461914
        },
        "model.layers.19.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.295316696166992
        },
        "model.layers.20.self_attn.q_proj": {
            "bit_width": 4,
            "error": 14.651766777038574
        },
        "model.layers.20.self_attn.k_proj": {
            "bit_width": 4,
            "error": 15.37707233428955
        },
        "model.layers.20.self_attn.v_proj": {
            "bit_width": 4,
            "error": 18.12177848815918
        },
        "model.layers.20.self_attn.o_proj": {
            "bit_width": 4,
            "error": 17.032392501831055
        },
        "model.layers.20.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.123899459838867
        },
        "model.layers.20.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.8698787689209
        },
        "model.layers.20.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.329376220703125
        },
        "model.layers.21.self_attn.q_proj": {
            "bit_width": 4,
            "error": 17.190574645996094
        },
        "model.layers.21.self_attn.k_proj": {
            "bit_width": 4,
            "error": 17.090070724487305
        },
        "model.layers.21.self_attn.v_proj": {
            "bit_width": 3,
            "error": 12.074166297912598
        },
        "model.layers.21.self_attn.o_proj": {
            "bit_width": 4,
            "error": 16.913053512573242
        },
        "model.layers.21.mlp.gate_proj": {
            "bit_width": 4,
            "error": 15.587900161743164
        },
        "model.layers.21.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.400915145874023
        },
        "model.layers.21.mlp.down_proj": {
            "bit_width": 4,
            "error": 16.674741744995117
        },
        "model.layers.22.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.6594181060791
        },
        "model.layers.22.self_attn.k_proj": {
            "bit_width": 4,
            "error": 17.818395614624023
        },
        "model.layers.22.self_attn.v_proj": {
            "bit_width": 4,
            "error": 17.780717849731445
        },
        "model.layers.22.self_attn.o_proj": {
            "bit_width": 4,
            "error": 17.331796646118164
        },
        "model.layers.22.mlp.gate_proj": {
            "bit_width": 4,
            "error": 15.883130073547363
        },
        "model.layers.22.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.51136589050293
        },
        "model.layers.22.mlp.down_proj": {
            "bit_width": 4,
            "error": 16.071794509887695
        },
        "model.layers.23.self_attn.q_proj": {
            "bit_width": 4,
            "error": 17.237060546875
        },
        "model.layers.23.self_attn.k_proj": {
            "bit_width": 4,
            "error": 17.803817749023438
        },
        "model.layers.23.self_attn.v_proj": {
            "bit_width": 4,
            "error": 17.48038673400879
        },
        "model.layers.23.self_attn.o_proj": {
            "bit_width": 3,
            "error": 12.382413864135742
        },
        "model.layers.23.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.094432830810547
        },
        "model.layers.23.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.818359375
        },
        "model.layers.23.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.11949920654297
        },
        "model.layers.24.self_attn.q_proj": {
            "bit_width": 4,
            "error": 17.309877395629883
        },
        "model.layers.24.self_attn.k_proj": {
            "bit_width": 4,
            "error": 16.891706466674805
        },
        "model.layers.24.self_attn.v_proj": {
            "bit_width": 4,
            "error": 18.188156127929688
        },
        "model.layers.24.self_attn.o_proj": {
            "bit_width": 4,
            "error": 17.582651138305664
        },
        "model.layers.24.mlp.gate_proj": {
            "bit_width": 4,
            "error": 15.811844825744629
        },
        "model.layers.24.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.010765075683594
        },
        "model.layers.24.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.838695526123047
        },
        "model.layers.25.self_attn.q_proj": {
            "bit_width": 4,
            "error": 17.18492317199707
        },
        "model.layers.25.self_attn.k_proj": {
            "bit_width": 4,
            "error": 18.6025447845459
        },
        "model.layers.25.self_attn.v_proj": {
            "bit_width": 4,
            "error": 17.95885467529297
        },
        "model.layers.25.self_attn.o_proj": {
            "bit_width": 3,
            "error": 13.174352645874023
        },
        "model.layers.25.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.118789672851562
        },
        "model.layers.25.mlp.up_proj": {
            "bit_width": 4,
            "error": 17.268251419067383
        },
        "model.layers.25.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.74259376525879
        },
        "model.layers.26.self_attn.q_proj": {
            "bit_width": 4,
            "error": 17.62250518798828
        },
        "model.layers.26.self_attn.k_proj": {
            "bit_width": 4,
            "error": 18.31281852722168
        },
        "model.layers.26.self_attn.v_proj": {
            "bit_width": 4,
            "error": 18.088756561279297
        },
        "model.layers.26.self_attn.o_proj": {
            "bit_width": 3,
            "error": 14.110384941101074
        },
        "model.layers.26.mlp.gate_proj": {
            "bit_width": 4,
            "error": 15.687620162963867
        },
        "model.layers.26.mlp.up_proj": {
            "bit_width": 4,
            "error": 17.127277374267578
        },
        "model.layers.26.mlp.down_proj": {
            "bit_width": 4,
            "error": 13.771830558776855
        },
        "model.layers.27.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.87972640991211
        },
        "model.layers.27.self_attn.k_proj": {
            "bit_width": 4,
            "error": 15.337028503417969
        },
        "model.layers.27.self_attn.v_proj": {
            "bit_width": 4,
            "error": 18.152915954589844
        },
        "model.layers.27.self_attn.o_proj": {
            "bit_width": 3,
            "error": 12.830873489379883
        },
        "model.layers.27.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.419628143310547
        },
        "model.layers.27.mlp.up_proj": {
            "bit_width": 4,
            "error": 17.005624771118164
        },
        "model.layers.27.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.723812103271484
        },
        "model.layers.28.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.108993530273438
        },
        "model.layers.28.self_attn.k_proj": {
            "bit_width": 4,
            "error": 16.0283203125
        },
        "model.layers.28.self_attn.v_proj": {
            "bit_width": 4,
            "error": 17.738014221191406
        },
        "model.layers.28.self_attn.o_proj": {
            "bit_width": 3,
            "error": 12.051558494567871
        },
        "model.layers.28.mlp.gate_proj": {
            "bit_width": 5,
            "error": 14.959879875183105
        },
        "model.layers.28.mlp.up_proj": {
            "bit_width": 5,
            "error": 17.50047492980957
        },
        "model.layers.28.mlp.down_proj": {
            "bit_width": 5,
            "error": 17.225982666015625
        },
        "model.layers.29.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.0274658203125
        },
        "model.layers.29.self_attn.k_proj": {
            "bit_width": 4,
            "error": 17.049333572387695
        },
        "model.layers.29.self_attn.v_proj": {
            "bit_width": 4,
            "error": 17.54867172241211
        },
        "model.layers.29.self_attn.o_proj": {
            "bit_width": 4,
            "error": 18.293643951416016
        },
        "model.layers.29.mlp.gate_proj": {
            "bit_width": 4,
            "error": 15.263943672180176
        },
        "model.layers.29.mlp.up_proj": {
            "bit_width": 5,
            "error": 17.02048110961914
        },
        "model.layers.29.mlp.down_proj": {
            "bit_width": 4,
            "error": 12.460551261901855
        },
        "lm_head": {
            "bit_width": 3,
            "error": 12.057417869567871
        }
    },
    "average_bit_width": 3.919431279620853,
    "error_threshold": 12,
    "min_quantile": 0,
    "max_quantile": 1,
    "quantized_model_benchmarks": {
        "wikitext_accuracy": 0.29380300065231574,
        "mmlu_results": {
            "overall_score": 0.3006134969325153,
            "task_scores": "{\"Task\":{\"0\":\"business_ethics\",\"1\":\"medical_genetics\",\"2\":\"formal_logic\"},\"Score\":{\"0\":0.31,\"1\":0.31,\"2\":0.2857142857}}"
        },
        "sanity_check_string": "user\nTell a short story of humanity with happy ending\n\nH% IH\n\nH%\n\nIH\n\nIH\n\nH\n\nH\n\nH\n\nH\n\nH\n\nH\n\nH\n\nH\n\n\nH\n\n\nH\n\n\nH\n\n\nH\n\n\nH\n\n\nH\n\n\nH\n\n\nH\n\n\nH\n\n\nH\n\n\nH\n\n\nH\n\n\nH\n\n\nH\n\n\n"
    }
}