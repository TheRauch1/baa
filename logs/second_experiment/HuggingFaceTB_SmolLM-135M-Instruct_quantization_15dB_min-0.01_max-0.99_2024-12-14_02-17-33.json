{
    "model_name": "HuggingFaceTB/SmolLM-135M-Instruct",
    "layerwise_quantization_info": {
        "model.layers.0.self_attn.q_proj": {
            "bit_width": 3,
            "error": 17.925222396850586
        },
        "model.layers.0.self_attn.k_proj": {
            "bit_width": 3,
            "error": 19.726917266845703
        },
        "model.layers.0.self_attn.v_proj": {
            "bit_width": 3,
            "error": 17.214258193969727
        },
        "model.layers.0.self_attn.o_proj": {
            "bit_width": 2,
            "error": 15.797410011291504
        },
        "model.layers.0.mlp.gate_proj": {
            "bit_width": 3,
            "error": 16.198230743408203
        },
        "model.layers.0.mlp.up_proj": {
            "bit_width": 4,
            "error": 19.92656898498535
        },
        "model.layers.0.mlp.down_proj": {
            "bit_width": 3,
            "error": 19.65424919128418
        },
        "model.layers.1.self_attn.q_proj": {
            "bit_width": 4,
            "error": 21.404115676879883
        },
        "model.layers.1.self_attn.k_proj": {
            "bit_width": 4,
            "error": 20.717792510986328
        },
        "model.layers.1.self_attn.v_proj": {
            "bit_width": 4,
            "error": 21.54695701599121
        },
        "model.layers.1.self_attn.o_proj": {
            "bit_width": 3,
            "error": 20.79230308532715
        },
        "model.layers.1.mlp.gate_proj": {
            "bit_width": 3,
            "error": 15.989030838012695
        },
        "model.layers.1.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.494537353515625
        },
        "model.layers.1.mlp.down_proj": {
            "bit_width": 3,
            "error": 18.201297760009766
        },
        "model.layers.2.self_attn.q_proj": {
            "bit_width": 4,
            "error": 20.76786231994629
        },
        "model.layers.2.self_attn.k_proj": {
            "bit_width": 4,
            "error": 20.19504165649414
        },
        "model.layers.2.self_attn.v_proj": {
            "bit_width": 4,
            "error": 20.998519897460938
        },
        "model.layers.2.self_attn.o_proj": {
            "bit_width": 4,
            "error": 21.17427635192871
        },
        "model.layers.2.mlp.gate_proj": {
            "bit_width": 3,
            "error": 15.894946098327637
        },
        "model.layers.2.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.173723220825195
        },
        "model.layers.2.mlp.down_proj": {
            "bit_width": 3,
            "error": 17.954967498779297
        },
        "model.layers.3.self_attn.q_proj": {
            "bit_width": 4,
            "error": 21.174728393554688
        },
        "model.layers.3.self_attn.k_proj": {
            "bit_width": 4,
            "error": 20.630348205566406
        },
        "model.layers.3.self_attn.v_proj": {
            "bit_width": 4,
            "error": 21.121557235717773
        },
        "model.layers.3.self_attn.o_proj": {
            "bit_width": 4,
            "error": 21.44920539855957
        },
        "model.layers.3.mlp.gate_proj": {
            "bit_width": 3,
            "error": 15.530835151672363
        },
        "model.layers.3.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.0014705657959
        },
        "model.layers.3.mlp.down_proj": {
            "bit_width": 3,
            "error": 16.206180572509766
        },
        "model.layers.4.self_attn.q_proj": {
            "bit_width": 4,
            "error": 21.279014587402344
        },
        "model.layers.4.self_attn.k_proj": {
            "bit_width": 4,
            "error": 21.143051147460938
        },
        "model.layers.4.self_attn.v_proj": {
            "bit_width": 4,
            "error": 21.540653228759766
        },
        "model.layers.4.self_attn.o_proj": {
            "bit_width": 4,
            "error": 20.45206069946289
        },
        "model.layers.4.mlp.gate_proj": {
            "bit_width": 3,
            "error": 16.034212112426758
        },
        "model.layers.4.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.047340393066406
        },
        "model.layers.4.mlp.down_proj": {
            "bit_width": 4,
            "error": 21.312496185302734
        },
        "model.layers.5.self_attn.q_proj": {
            "bit_width": 4,
            "error": 21.44310760498047
        },
        "model.layers.5.self_attn.k_proj": {
            "bit_width": 3,
            "error": 15.157041549682617
        },
        "model.layers.5.self_attn.v_proj": {
            "bit_width": 3,
            "error": 15.305538177490234
        },
        "model.layers.5.self_attn.o_proj": {
            "bit_width": 4,
            "error": 21.219146728515625
        },
        "model.layers.5.mlp.gate_proj": {
            "bit_width": 3,
            "error": 15.95036506652832
        },
        "model.layers.5.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.243247985839844
        },
        "model.layers.5.mlp.down_proj": {
            "bit_width": 4,
            "error": 21.531829833984375
        },
        "model.layers.6.self_attn.q_proj": {
            "bit_width": 4,
            "error": 21.32282066345215
        },
        "model.layers.6.self_attn.k_proj": {
            "bit_width": 4,
            "error": 20.707199096679688
        },
        "model.layers.6.self_attn.v_proj": {
            "bit_width": 3,
            "error": 15.136455535888672
        },
        "model.layers.6.self_attn.o_proj": {
            "bit_width": 4,
            "error": 21.3050537109375
        },
        "model.layers.6.mlp.gate_proj": {
            "bit_width": 3,
            "error": 15.8317289352417
        },
        "model.layers.6.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.23990821838379
        },
        "model.layers.6.mlp.down_proj": {
            "bit_width": 4,
            "error": 21.527400970458984
        },
        "model.layers.7.self_attn.q_proj": {
            "bit_width": 3,
            "error": 15.025467872619629
        },
        "model.layers.7.self_attn.k_proj": {
            "bit_width": 4,
            "error": 20.188297271728516
        },
        "model.layers.7.self_attn.v_proj": {
            "bit_width": 4,
            "error": 21.65043067932129
        },
        "model.layers.7.self_attn.o_proj": {
            "bit_width": 4,
            "error": 21.610389709472656
        },
        "model.layers.7.mlp.gate_proj": {
            "bit_width": 3,
            "error": 15.986316680908203
        },
        "model.layers.7.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.226282119750977
        },
        "model.layers.7.mlp.down_proj": {
            "bit_width": 3,
            "error": 16.009096145629883
        },
        "model.layers.8.self_attn.q_proj": {
            "bit_width": 4,
            "error": 19.674409866333008
        },
        "model.layers.8.self_attn.k_proj": {
            "bit_width": 3,
            "error": 15.817709922790527
        },
        "model.layers.8.self_attn.v_proj": {
            "bit_width": 4,
            "error": 21.486900329589844
        },
        "model.layers.8.self_attn.o_proj": {
            "bit_width": 4,
            "error": 21.390771865844727
        },
        "model.layers.8.mlp.gate_proj": {
            "bit_width": 3,
            "error": 15.93253231048584
        },
        "model.layers.8.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.239030838012695
        },
        "model.layers.8.mlp.down_proj": {
            "bit_width": 3,
            "error": 15.295305252075195
        },
        "model.layers.9.self_attn.q_proj": {
            "bit_width": 4,
            "error": 21.361785888671875
        },
        "model.layers.9.self_attn.k_proj": {
            "bit_width": 4,
            "error": 19.79599380493164
        },
        "model.layers.9.self_attn.v_proj": {
            "bit_width": 4,
            "error": 21.378341674804688
        },
        "model.layers.9.self_attn.o_proj": {
            "bit_width": 4,
            "error": 20.021587371826172
        },
        "model.layers.9.mlp.gate_proj": {
            "bit_width": 3,
            "error": 15.707042694091797
        },
        "model.layers.9.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.26146125793457
        },
        "model.layers.9.mlp.down_proj": {
            "bit_width": 4,
            "error": 21.545974731445312
        },
        "model.layers.10.self_attn.q_proj": {
            "bit_width": 4,
            "error": 19.799057006835938
        },
        "model.layers.10.self_attn.k_proj": {
            "bit_width": 4,
            "error": 21.10418128967285
        },
        "model.layers.10.self_attn.v_proj": {
            "bit_width": 4,
            "error": 21.020858764648438
        },
        "model.layers.10.self_attn.o_proj": {
            "bit_width": 3,
            "error": 15.033544540405273
        },
        "model.layers.10.mlp.gate_proj": {
            "bit_width": 3,
            "error": 16.75543212890625
        },
        "model.layers.10.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.180862426757812
        },
        "model.layers.10.mlp.down_proj": {
            "bit_width": 4,
            "error": 21.470901489257812
        },
        "model.layers.11.self_attn.q_proj": {
            "bit_width": 4,
            "error": 21.77124786376953
        },
        "model.layers.11.self_attn.k_proj": {
            "bit_width": 4,
            "error": 21.112777709960938
        },
        "model.layers.11.self_attn.v_proj": {
            "bit_width": 3,
            "error": 15.045313835144043
        },
        "model.layers.11.self_attn.o_proj": {
            "bit_width": 4,
            "error": 21.095617294311523
        },
        "model.layers.11.mlp.gate_proj": {
            "bit_width": 3,
            "error": 16.12340545654297
        },
        "model.layers.11.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.224634170532227
        },
        "model.layers.11.mlp.down_proj": {
            "bit_width": 2,
            "error": 31.916532516479492
        },
        "model.layers.12.self_attn.q_proj": {
            "bit_width": 4,
            "error": 20.38863182067871
        },
        "model.layers.12.self_attn.k_proj": {
            "bit_width": 4,
            "error": 20.982746124267578
        },
        "model.layers.12.self_attn.v_proj": {
            "bit_width": 3,
            "error": 15.07402515411377
        },
        "model.layers.12.self_attn.o_proj": {
            "bit_width": 4,
            "error": 20.225093841552734
        },
        "model.layers.12.mlp.gate_proj": {
            "bit_width": 3,
            "error": 15.181867599487305
        },
        "model.layers.12.mlp.up_proj": {
            "bit_width": 3,
            "error": 15.114503860473633
        },
        "model.layers.12.mlp.down_proj": {
            "bit_width": 3,
            "error": 18.28803062438965
        },
        "model.layers.13.self_attn.q_proj": {
            "bit_width": 4,
            "error": 19.06618309020996
        },
        "model.layers.13.self_attn.k_proj": {
            "bit_width": 3,
            "error": 15.25076961517334
        },
        "model.layers.13.self_attn.v_proj": {
            "bit_width": 3,
            "error": 15.741776466369629
        },
        "model.layers.13.self_attn.o_proj": {
            "bit_width": 3,
            "error": 15.91999626159668
        },
        "model.layers.13.mlp.gate_proj": {
            "bit_width": 3,
            "error": 15.764599800109863
        },
        "model.layers.13.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.202075958251953
        },
        "model.layers.13.mlp.down_proj": {
            "bit_width": 4,
            "error": 21.528141021728516
        },
        "model.layers.14.self_attn.q_proj": {
            "bit_width": 4,
            "error": 20.743915557861328
        },
        "model.layers.14.self_attn.k_proj": {
            "bit_width": 4,
            "error": 21.724056243896484
        },
        "model.layers.14.self_attn.v_proj": {
            "bit_width": 3,
            "error": 15.399986267089844
        },
        "model.layers.14.self_attn.o_proj": {
            "bit_width": 3,
            "error": 15.714580535888672
        },
        "model.layers.14.mlp.gate_proj": {
            "bit_width": 4,
            "error": 20.987932205200195
        },
        "model.layers.14.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.13461685180664
        },
        "model.layers.14.mlp.down_proj": {
            "bit_width": 4,
            "error": 21.068592071533203
        },
        "model.layers.15.self_attn.q_proj": {
            "bit_width": 4,
            "error": 19.035438537597656
        },
        "model.layers.15.self_attn.k_proj": {
            "bit_width": 3,
            "error": 16.288972854614258
        },
        "model.layers.15.self_attn.v_proj": {
            "bit_width": 3,
            "error": 15.234795570373535
        },
        "model.layers.15.self_attn.o_proj": {
            "bit_width": 4,
            "error": 19.75653076171875
        },
        "model.layers.15.mlp.gate_proj": {
            "bit_width": 4,
            "error": 21.423303604125977
        },
        "model.layers.15.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.098691940307617
        },
        "model.layers.15.mlp.down_proj": {
            "bit_width": 4,
            "error": 21.075294494628906
        },
        "model.layers.16.self_attn.q_proj": {
            "bit_width": 4,
            "error": 19.559921264648438
        },
        "model.layers.16.self_attn.k_proj": {
            "bit_width": 4,
            "error": 21.61475944519043
        },
        "model.layers.16.self_attn.v_proj": {
            "bit_width": 4,
            "error": 21.343965530395508
        },
        "model.layers.16.self_attn.o_proj": {
            "bit_width": 3,
            "error": 15.505672454833984
        },
        "model.layers.16.mlp.gate_proj": {
            "bit_width": 4,
            "error": 21.435474395751953
        },
        "model.layers.16.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.347755432128906
        },
        "model.layers.16.mlp.down_proj": {
            "bit_width": 4,
            "error": 21.137165069580078
        },
        "model.layers.17.self_attn.q_proj": {
            "bit_width": 4,
            "error": 19.25632095336914
        },
        "model.layers.17.self_attn.k_proj": {
            "bit_width": 4,
            "error": 21.803775787353516
        },
        "model.layers.17.self_attn.v_proj": {
            "bit_width": 3,
            "error": 15.164005279541016
        },
        "model.layers.17.self_attn.o_proj": {
            "bit_width": 4,
            "error": 20.697044372558594
        },
        "model.layers.17.mlp.gate_proj": {
            "bit_width": 3,
            "error": 15.046977043151855
        },
        "model.layers.17.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.38352394104004
        },
        "model.layers.17.mlp.down_proj": {
            "bit_width": 4,
            "error": 21.32457733154297
        },
        "model.layers.18.self_attn.q_proj": {
            "bit_width": 4,
            "error": 20.498851776123047
        },
        "model.layers.18.self_attn.k_proj": {
            "bit_width": 4,
            "error": 21.10927391052246
        },
        "model.layers.18.self_attn.v_proj": {
            "bit_width": 4,
            "error": 20.421859741210938
        },
        "model.layers.18.self_attn.o_proj": {
            "bit_width": 3,
            "error": 16.016572952270508
        },
        "model.layers.18.mlp.gate_proj": {
            "bit_width": 3,
            "error": 15.119556427001953
        },
        "model.layers.18.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.245243072509766
        },
        "model.layers.18.mlp.down_proj": {
            "bit_width": 4,
            "error": 21.038904190063477
        },
        "model.layers.19.self_attn.q_proj": {
            "bit_width": 4,
            "error": 19.754295349121094
        },
        "model.layers.19.self_attn.k_proj": {
            "bit_width": 3,
            "error": 15.917442321777344
        },
        "model.layers.19.self_attn.v_proj": {
            "bit_width": 4,
            "error": 21.37151527404785
        },
        "model.layers.19.self_attn.o_proj": {
            "bit_width": 3,
            "error": 15.779060363769531
        },
        "model.layers.19.mlp.gate_proj": {
            "bit_width": 3,
            "error": 15.365137100219727
        },
        "model.layers.19.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.411067962646484
        },
        "model.layers.19.mlp.down_proj": {
            "bit_width": 4,
            "error": 21.080970764160156
        },
        "model.layers.20.self_attn.q_proj": {
            "bit_width": 4,
            "error": 19.652585983276367
        },
        "model.layers.20.self_attn.k_proj": {
            "bit_width": 3,
            "error": 15.50655746459961
        },
        "model.layers.20.self_attn.v_proj": {
            "bit_width": 4,
            "error": 20.81296157836914
        },
        "model.layers.20.self_attn.o_proj": {
            "bit_width": 4,
            "error": 21.438003540039062
        },
        "model.layers.20.mlp.gate_proj": {
            "bit_width": 3,
            "error": 15.578529357910156
        },
        "model.layers.20.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.6567440032959
        },
        "model.layers.20.mlp.down_proj": {
            "bit_width": 4,
            "error": 21.386497497558594
        },
        "model.layers.21.self_attn.q_proj": {
            "bit_width": 4,
            "error": 20.812618255615234
        },
        "model.layers.21.self_attn.k_proj": {
            "bit_width": 3,
            "error": 15.023159980773926
        },
        "model.layers.21.self_attn.v_proj": {
            "bit_width": 4,
            "error": 20.75383186340332
        },
        "model.layers.21.self_attn.o_proj": {
            "bit_width": 4,
            "error": 21.080053329467773
        },
        "model.layers.21.mlp.gate_proj": {
            "bit_width": 3,
            "error": 15.580903053283691
        },
        "model.layers.21.mlp.up_proj": {
            "bit_width": 3,
            "error": 15.122844696044922
        },
        "model.layers.21.mlp.down_proj": {
            "bit_width": 4,
            "error": 21.545799255371094
        },
        "model.layers.22.self_attn.q_proj": {
            "bit_width": 4,
            "error": 21.297433853149414
        },
        "model.layers.22.self_attn.k_proj": {
            "bit_width": 3,
            "error": 15.712930679321289
        },
        "model.layers.22.self_attn.v_proj": {
            "bit_width": 4,
            "error": 20.620813369750977
        },
        "model.layers.22.self_attn.o_proj": {
            "bit_width": 3,
            "error": 18.576801300048828
        },
        "model.layers.22.mlp.gate_proj": {
            "bit_width": 3,
            "error": 15.238615036010742
        },
        "model.layers.22.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.43710708618164
        },
        "model.layers.22.mlp.down_proj": {
            "bit_width": 4,
            "error": 21.531055450439453
        },
        "model.layers.23.self_attn.q_proj": {
            "bit_width": 4,
            "error": 20.9334659576416
        },
        "model.layers.23.self_attn.k_proj": {
            "bit_width": 3,
            "error": 15.169336318969727
        },
        "model.layers.23.self_attn.v_proj": {
            "bit_width": 4,
            "error": 19.759172439575195
        },
        "model.layers.23.self_attn.o_proj": {
            "bit_width": 3,
            "error": 18.79959487915039
        },
        "model.layers.23.mlp.gate_proj": {
            "bit_width": 4,
            "error": 21.581100463867188
        },
        "model.layers.23.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.212963104248047
        },
        "model.layers.23.mlp.down_proj": {
            "bit_width": 4,
            "error": 21.34284782409668
        },
        "model.layers.24.self_attn.q_proj": {
            "bit_width": 3,
            "error": 15.147581100463867
        },
        "model.layers.24.self_attn.k_proj": {
            "bit_width": 3,
            "error": 16.98885154724121
        },
        "model.layers.24.self_attn.v_proj": {
            "bit_width": 4,
            "error": 20.496978759765625
        },
        "model.layers.24.self_attn.o_proj": {
            "bit_width": 3,
            "error": 17.284557342529297
        },
        "model.layers.24.mlp.gate_proj": {
            "bit_width": 4,
            "error": 21.51614761352539
        },
        "model.layers.24.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.419170379638672
        },
        "model.layers.24.mlp.down_proj": {
            "bit_width": 4,
            "error": 21.316757202148438
        },
        "model.layers.25.self_attn.q_proj": {
            "bit_width": 4,
            "error": 21.34294891357422
        },
        "model.layers.25.self_attn.k_proj": {
            "bit_width": 4,
            "error": 21.88011360168457
        },
        "model.layers.25.self_attn.v_proj": {
            "bit_width": 4,
            "error": 20.20646858215332
        },
        "model.layers.25.self_attn.o_proj": {
            "bit_width": 3,
            "error": 18.849597930908203
        },
        "model.layers.25.mlp.gate_proj": {
            "bit_width": 3,
            "error": 15.167146682739258
        },
        "model.layers.25.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.34159278869629
        },
        "model.layers.25.mlp.down_proj": {
            "bit_width": 4,
            "error": 21.282997131347656
        },
        "model.layers.26.self_attn.q_proj": {
            "bit_width": 4,
            "error": 21.221561431884766
        },
        "model.layers.26.self_attn.k_proj": {
            "bit_width": 4,
            "error": 21.006784439086914
        },
        "model.layers.26.self_attn.v_proj": {
            "bit_width": 4,
            "error": 20.210582733154297
        },
        "model.layers.26.self_attn.o_proj": {
            "bit_width": 3,
            "error": 22.157516479492188
        },
        "model.layers.26.mlp.gate_proj": {
            "bit_width": 3,
            "error": 15.080498695373535
        },
        "model.layers.26.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.533912658691406
        },
        "model.layers.26.mlp.down_proj": {
            "bit_width": 3,
            "error": 15.177827835083008
        },
        "model.layers.27.self_attn.q_proj": {
            "bit_width": 4,
            "error": 19.942758560180664
        },
        "model.layers.27.self_attn.k_proj": {
            "bit_width": 3,
            "error": 16.595869064331055
        },
        "model.layers.27.self_attn.v_proj": {
            "bit_width": 4,
            "error": 20.8322696685791
        },
        "model.layers.27.self_attn.o_proj": {
            "bit_width": 3,
            "error": 18.224580764770508
        },
        "model.layers.27.mlp.gate_proj": {
            "bit_width": 3,
            "error": 15.039665222167969
        },
        "model.layers.27.mlp.up_proj": {
            "bit_width": 4,
            "error": 21.39044189453125
        },
        "model.layers.27.mlp.down_proj": {
            "bit_width": 4,
            "error": 21.337169647216797
        },
        "model.layers.28.self_attn.q_proj": {
            "bit_width": 4,
            "error": 20.752803802490234
        },
        "model.layers.28.self_attn.k_proj": {
            "bit_width": 3,
            "error": 16.45274543762207
        },
        "model.layers.28.self_attn.v_proj": {
            "bit_width": 4,
            "error": 20.796415328979492
        },
        "model.layers.28.self_attn.o_proj": {
            "bit_width": 3,
            "error": 16.485605239868164
        },
        "model.layers.28.mlp.gate_proj": {
            "bit_width": 3,
            "error": 17.42892837524414
        },
        "model.layers.28.mlp.up_proj": {
            "bit_width": 3,
            "error": 15.575742721557617
        },
        "model.layers.28.mlp.down_proj": {
            "bit_width": 2,
            "error": 21.367839813232422
        },
        "model.layers.29.self_attn.q_proj": {
            "bit_width": 4,
            "error": 19.288410186767578
        },
        "model.layers.29.self_attn.k_proj": {
            "bit_width": 4,
            "error": 20.14923095703125
        },
        "model.layers.29.self_attn.v_proj": {
            "bit_width": 4,
            "error": 19.93856430053711
        },
        "model.layers.29.self_attn.o_proj": {
            "bit_width": 3,
            "error": 17.02105712890625
        },
        "model.layers.29.mlp.gate_proj": {
            "bit_width": 4,
            "error": 19.905733108520508
        },
        "model.layers.29.mlp.up_proj": {
            "bit_width": 3,
            "error": 15.298004150390625
        },
        "model.layers.29.mlp.down_proj": {
            "bit_width": 3,
            "error": 17.327835083007812
        },
        "lm_head": {
            "bit_width": 3,
            "error": 18.54452133178711
        }
    },
    "average_bit_width": 3.6018957345971563,
    "error_threshold": 15,
    "min_quantile": 0.01,
    "max_quantile": 0.99,
    "quantized_model_benchmarks": {
        "wikitext_accuracy": 0.3851706892802783,
        "mmlu_results": {
            "overall_score": 0.294478527607362,
            "task_scores": "{\"Task\":{\"0\":\"business_ethics\",\"1\":\"medical_genetics\",\"2\":\"formal_logic\"},\"Score\":{\"0\":0.3,\"1\":0.3,\"2\":0.2857142857}}"
        },
        "sanity_check_string": "user\nTell a short story of humanity with happy ending\n\n**The Last Hope**\n\nThe sun had long since faded, leaving behind a glimmer of hope for humanity to come. A great nation, a civilization, a civilization, where we were, where we were, where we were, and where we were, we have taken a wrong turn, and a wrong heart, and a wrong choice, to give life a second chance.\n\nWe would like to pause, to slow our slavish expression, but I am here, like"
    }
}