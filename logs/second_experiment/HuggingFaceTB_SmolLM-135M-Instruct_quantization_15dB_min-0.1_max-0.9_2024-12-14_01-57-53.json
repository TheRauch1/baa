{
    "model_name": "HuggingFaceTB/SmolLM-135M-Instruct",
    "layerwise_quantization_info": {
        "model.layers.0.self_attn.q_proj": {
            "bit_width": 2,
            "error": 17.39605140686035
        },
        "model.layers.0.self_attn.k_proj": {
            "bit_width": 2,
            "error": 17.952112197875977
        },
        "model.layers.0.self_attn.v_proj": {
            "bit_width": 2,
            "error": 17.60275650024414
        },
        "model.layers.0.self_attn.o_proj": {
            "bit_width": 2,
            "error": 23.435375213623047
        },
        "model.layers.0.mlp.gate_proj": {
            "bit_width": 3,
            "error": 22.27780532836914
        },
        "model.layers.0.mlp.up_proj": {
            "bit_width": 3,
            "error": 19.01935386657715
        },
        "model.layers.0.mlp.down_proj": {
            "bit_width": 2,
            "error": 19.49837875366211
        },
        "model.layers.1.self_attn.q_proj": {
            "bit_width": 2,
            "error": 15.731910705566406
        },
        "model.layers.1.self_attn.k_proj": {
            "bit_width": 2,
            "error": 15.264596939086914
        },
        "model.layers.1.self_attn.v_proj": {
            "bit_width": 3,
            "error": 21.81657600402832
        },
        "model.layers.1.self_attn.o_proj": {
            "bit_width": 2,
            "error": 19.248172760009766
        },
        "model.layers.1.mlp.gate_proj": {
            "bit_width": 3,
            "error": 21.3958740234375
        },
        "model.layers.1.mlp.up_proj": {
            "bit_width": 3,
            "error": 20.116159439086914
        },
        "model.layers.1.mlp.down_proj": {
            "bit_width": 2,
            "error": 16.253210067749023
        },
        "model.layers.2.self_attn.q_proj": {
            "bit_width": 2,
            "error": 15.278087615966797
        },
        "model.layers.2.self_attn.k_proj": {
            "bit_width": 2,
            "error": 16.568252563476562
        },
        "model.layers.2.self_attn.v_proj": {
            "bit_width": 3,
            "error": 20.258052825927734
        },
        "model.layers.2.self_attn.o_proj": {
            "bit_width": 3,
            "error": 20.09157943725586
        },
        "model.layers.2.mlp.gate_proj": {
            "bit_width": 3,
            "error": 21.392932891845703
        },
        "model.layers.2.mlp.up_proj": {
            "bit_width": 3,
            "error": 19.938655853271484
        },
        "model.layers.2.mlp.down_proj": {
            "bit_width": 2,
            "error": 16.061710357666016
        },
        "model.layers.3.self_attn.q_proj": {
            "bit_width": 3,
            "error": 22.221847534179688
        },
        "model.layers.3.self_attn.k_proj": {
            "bit_width": 2,
            "error": 15.430766105651855
        },
        "model.layers.3.self_attn.v_proj": {
            "bit_width": 3,
            "error": 20.1398868560791
        },
        "model.layers.3.self_attn.o_proj": {
            "bit_width": 3,
            "error": 20.26828956604004
        },
        "model.layers.3.mlp.gate_proj": {
            "bit_width": 3,
            "error": 20.84305191040039
        },
        "model.layers.3.mlp.up_proj": {
            "bit_width": 3,
            "error": 19.68075942993164
        },
        "model.layers.3.mlp.down_proj": {
            "bit_width": 3,
            "error": 21.470823287963867
        },
        "model.layers.4.self_attn.q_proj": {
            "bit_width": 2,
            "error": 15.571290016174316
        },
        "model.layers.4.self_attn.k_proj": {
            "bit_width": 2,
            "error": 15.808418273925781
        },
        "model.layers.4.self_attn.v_proj": {
            "bit_width": 3,
            "error": 20.617521286010742
        },
        "model.layers.4.self_attn.o_proj": {
            "bit_width": 3,
            "error": 19.53426742553711
        },
        "model.layers.4.mlp.gate_proj": {
            "bit_width": 3,
            "error": 21.416349411010742
        },
        "model.layers.4.mlp.up_proj": {
            "bit_width": 3,
            "error": 19.720596313476562
        },
        "model.layers.4.mlp.down_proj": {
            "bit_width": 3,
            "error": 19.981290817260742
        },
        "model.layers.5.self_attn.q_proj": {
            "bit_width": 2,
            "error": 15.262773513793945
        },
        "model.layers.5.self_attn.k_proj": {
            "bit_width": 2,
            "error": 15.557256698608398
        },
        "model.layers.5.self_attn.v_proj": {
            "bit_width": 3,
            "error": 20.541187286376953
        },
        "model.layers.5.self_attn.o_proj": {
            "bit_width": 3,
            "error": 19.854808807373047
        },
        "model.layers.5.mlp.gate_proj": {
            "bit_width": 3,
            "error": 21.247121810913086
        },
        "model.layers.5.mlp.up_proj": {
            "bit_width": 3,
            "error": 19.85615348815918
        },
        "model.layers.5.mlp.down_proj": {
            "bit_width": 3,
            "error": 20.05704116821289
        },
        "model.layers.6.self_attn.q_proj": {
            "bit_width": 2,
            "error": 15.002245903015137
        },
        "model.layers.6.self_attn.k_proj": {
            "bit_width": 2,
            "error": 15.531932830810547
        },
        "model.layers.6.self_attn.v_proj": {
            "bit_width": 3,
            "error": 20.470483779907227
        },
        "model.layers.6.self_attn.o_proj": {
            "bit_width": 3,
            "error": 19.55665397644043
        },
        "model.layers.6.mlp.gate_proj": {
            "bit_width": 3,
            "error": 21.141155242919922
        },
        "model.layers.6.mlp.up_proj": {
            "bit_width": 3,
            "error": 19.890377044677734
        },
        "model.layers.6.mlp.down_proj": {
            "bit_width": 3,
            "error": 20.146392822265625
        },
        "model.layers.7.self_attn.q_proj": {
            "bit_width": 3,
            "error": 21.9858455657959
        },
        "model.layers.7.self_attn.k_proj": {
            "bit_width": 2,
            "error": 15.772918701171875
        },
        "model.layers.7.self_attn.v_proj": {
            "bit_width": 3,
            "error": 20.054447174072266
        },
        "model.layers.7.self_attn.o_proj": {
            "bit_width": 3,
            "error": 20.13690757751465
        },
        "model.layers.7.mlp.gate_proj": {
            "bit_width": 3,
            "error": 21.793079376220703
        },
        "model.layers.7.mlp.up_proj": {
            "bit_width": 3,
            "error": 20.173786163330078
        },
        "model.layers.7.mlp.down_proj": {
            "bit_width": 3,
            "error": 21.273164749145508
        },
        "model.layers.8.self_attn.q_proj": {
            "bit_width": 2,
            "error": 15.290946006774902
        },
        "model.layers.8.self_attn.k_proj": {
            "bit_width": 2,
            "error": 15.454936981201172
        },
        "model.layers.8.self_attn.v_proj": {
            "bit_width": 3,
            "error": 20.607481002807617
        },
        "model.layers.8.self_attn.o_proj": {
            "bit_width": 3,
            "error": 20.094690322875977
        },
        "model.layers.8.mlp.gate_proj": {
            "bit_width": 3,
            "error": 21.61736488342285
        },
        "model.layers.8.mlp.up_proj": {
            "bit_width": 3,
            "error": 20.107929229736328
        },
        "model.layers.8.mlp.down_proj": {
            "bit_width": 3,
            "error": 20.556123733520508
        },
        "model.layers.9.self_attn.q_proj": {
            "bit_width": 2,
            "error": 15.379571914672852
        },
        "model.layers.9.self_attn.k_proj": {
            "bit_width": 2,
            "error": 15.048770904541016
        },
        "model.layers.9.self_attn.v_proj": {
            "bit_width": 3,
            "error": 21.60009765625
        },
        "model.layers.9.self_attn.o_proj": {
            "bit_width": 3,
            "error": 18.7983341217041
        },
        "model.layers.9.mlp.gate_proj": {
            "bit_width": 3,
            "error": 21.36720848083496
        },
        "model.layers.9.mlp.up_proj": {
            "bit_width": 3,
            "error": 20.119197845458984
        },
        "model.layers.9.mlp.down_proj": {
            "bit_width": 3,
            "error": 20.243942260742188
        },
        "model.layers.10.self_attn.q_proj": {
            "bit_width": 3,
            "error": 22.03697395324707
        },
        "model.layers.10.self_attn.k_proj": {
            "bit_width": 3,
            "error": 21.832372665405273
        },
        "model.layers.10.self_attn.v_proj": {
            "bit_width": 3,
            "error": 19.504684448242188
        },
        "model.layers.10.self_attn.o_proj": {
            "bit_width": 3,
            "error": 20.51418113708496
        },
        "model.layers.10.mlp.gate_proj": {
            "bit_width": 2,
            "error": 15.003778457641602
        },
        "model.layers.10.mlp.up_proj": {
            "bit_width": 3,
            "error": 20.141483306884766
        },
        "model.layers.10.mlp.down_proj": {
            "bit_width": 3,
            "error": 20.127696990966797
        },
        "model.layers.11.self_attn.q_proj": {
            "bit_width": 2,
            "error": 15.0504732131958
        },
        "model.layers.11.self_attn.k_proj": {
            "bit_width": 3,
            "error": 22.244741439819336
        },
        "model.layers.11.self_attn.v_proj": {
            "bit_width": 3,
            "error": 20.219585418701172
        },
        "model.layers.11.self_attn.o_proj": {
            "bit_width": 3,
            "error": 19.813154220581055
        },
        "model.layers.11.mlp.gate_proj": {
            "bit_width": 3,
            "error": 21.933984756469727
        },
        "model.layers.11.mlp.up_proj": {
            "bit_width": 3,
            "error": 20.271400451660156
        },
        "model.layers.11.mlp.down_proj": {
            "bit_width": 2,
            "error": 38.66587829589844
        },
        "model.layers.12.self_attn.q_proj": {
            "bit_width": 2,
            "error": 17.17546844482422
        },
        "model.layers.12.self_attn.k_proj": {
            "bit_width": 2,
            "error": 15.828592300415039
        },
        "model.layers.12.self_attn.v_proj": {
            "bit_width": 3,
            "error": 20.623891830444336
        },
        "model.layers.12.self_attn.o_proj": {
            "bit_width": 3,
            "error": 19.006168365478516
        },
        "model.layers.12.mlp.gate_proj": {
            "bit_width": 3,
            "error": 21.931079864501953
        },
        "model.layers.12.mlp.up_proj": {
            "bit_width": 3,
            "error": 21.536664962768555
        },
        "model.layers.12.mlp.down_proj": {
            "bit_width": 2,
            "error": 16.38690185546875
        },
        "model.layers.13.self_attn.q_proj": {
            "bit_width": 2,
            "error": 18.190767288208008
        },
        "model.layers.13.self_attn.k_proj": {
            "bit_width": 2,
            "error": 16.04193115234375
        },
        "model.layers.13.self_attn.v_proj": {
            "bit_width": 3,
            "error": 21.874265670776367
        },
        "model.layers.13.self_attn.o_proj": {
            "bit_width": 3,
            "error": 21.743261337280273
        },
        "model.layers.13.mlp.gate_proj": {
            "bit_width": 3,
            "error": 22.019638061523438
        },
        "model.layers.13.mlp.up_proj": {
            "bit_width": 3,
            "error": 20.533809661865234
        },
        "model.layers.13.mlp.down_proj": {
            "bit_width": 3,
            "error": 20.361270904541016
        },
        "model.layers.14.self_attn.q_proj": {
            "bit_width": 2,
            "error": 17.748830795288086
        },
        "model.layers.14.self_attn.k_proj": {
            "bit_width": 2,
            "error": 18.517786026000977
        },
        "model.layers.14.self_attn.v_proj": {
            "bit_width": 2,
            "error": 15.137205123901367
        },
        "model.layers.14.self_attn.o_proj": {
            "bit_width": 3,
            "error": 21.255523681640625
        },
        "model.layers.14.mlp.gate_proj": {
            "bit_width": 3,
            "error": 21.504150390625
        },
        "model.layers.14.mlp.up_proj": {
            "bit_width": 3,
            "error": 20.582599639892578
        },
        "model.layers.14.mlp.down_proj": {
            "bit_width": 3,
            "error": 20.063323974609375
        },
        "model.layers.15.self_attn.q_proj": {
            "bit_width": 2,
            "error": 16.571552276611328
        },
        "model.layers.15.self_attn.k_proj": {
            "bit_width": 2,
            "error": 17.636520385742188
        },
        "model.layers.15.self_attn.v_proj": {
            "bit_width": 3,
            "error": 20.606796264648438
        },
        "model.layers.15.self_attn.o_proj": {
            "bit_width": 3,
            "error": 18.896265029907227
        },
        "model.layers.15.mlp.gate_proj": {
            "bit_width": 3,
            "error": 21.363304138183594
        },
        "model.layers.15.mlp.up_proj": {
            "bit_width": 3,
            "error": 20.467491149902344
        },
        "model.layers.15.mlp.down_proj": {
            "bit_width": 3,
            "error": 20.007572174072266
        },
        "model.layers.16.self_attn.q_proj": {
            "bit_width": 2,
            "error": 17.201622009277344
        },
        "model.layers.16.self_attn.k_proj": {
            "bit_width": 2,
            "error": 18.154083251953125
        },
        "model.layers.16.self_attn.v_proj": {
            "bit_width": 3,
            "error": 19.925485610961914
        },
        "model.layers.16.self_attn.o_proj": {
            "bit_width": 3,
            "error": 21.10968017578125
        },
        "model.layers.16.mlp.gate_proj": {
            "bit_width": 3,
            "error": 21.673755645751953
        },
        "model.layers.16.mlp.up_proj": {
            "bit_width": 3,
            "error": 20.55998992919922
        },
        "model.layers.16.mlp.down_proj": {
            "bit_width": 3,
            "error": 19.938581466674805
        },
        "model.layers.17.self_attn.q_proj": {
            "bit_width": 2,
            "error": 17.222471237182617
        },
        "model.layers.17.self_attn.k_proj": {
            "bit_width": 2,
            "error": 17.462665557861328
        },
        "model.layers.17.self_attn.v_proj": {
            "bit_width": 3,
            "error": 20.68498992919922
        },
        "model.layers.17.self_attn.o_proj": {
            "bit_width": 3,
            "error": 19.7008056640625
        },
        "model.layers.17.mlp.gate_proj": {
            "bit_width": 3,
            "error": 21.46831512451172
        },
        "model.layers.17.mlp.up_proj": {
            "bit_width": 3,
            "error": 20.506359100341797
        },
        "model.layers.17.mlp.down_proj": {
            "bit_width": 3,
            "error": 20.14713478088379
        },
        "model.layers.18.self_attn.q_proj": {
            "bit_width": 2,
            "error": 17.325132369995117
        },
        "model.layers.18.self_attn.k_proj": {
            "bit_width": 2,
            "error": 18.42298126220703
        },
        "model.layers.18.self_attn.v_proj": {
            "bit_width": 3,
            "error": 20.124622344970703
        },
        "model.layers.18.self_attn.o_proj": {
            "bit_width": 3,
            "error": 21.607715606689453
        },
        "model.layers.18.mlp.gate_proj": {
            "bit_width": 3,
            "error": 22.070350646972656
        },
        "model.layers.18.mlp.up_proj": {
            "bit_width": 3,
            "error": 20.441133499145508
        },
        "model.layers.18.mlp.down_proj": {
            "bit_width": 3,
            "error": 19.953598022460938
        },
        "model.layers.19.self_attn.q_proj": {
            "bit_width": 2,
            "error": 17.57326889038086
        },
        "model.layers.19.self_attn.k_proj": {
            "bit_width": 2,
            "error": 17.376949310302734
        },
        "model.layers.19.self_attn.v_proj": {
            "bit_width": 3,
            "error": 20.066450119018555
        },
        "model.layers.19.self_attn.o_proj": {
            "bit_width": 3,
            "error": 21.4433650970459
        },
        "model.layers.19.mlp.gate_proj": {
            "bit_width": 3,
            "error": 21.95026397705078
        },
        "model.layers.19.mlp.up_proj": {
            "bit_width": 3,
            "error": 20.53462791442871
        },
        "model.layers.19.mlp.down_proj": {
            "bit_width": 3,
            "error": 19.826017379760742
        },
        "model.layers.20.self_attn.q_proj": {
            "bit_width": 2,
            "error": 17.714414596557617
        },
        "model.layers.20.self_attn.k_proj": {
            "bit_width": 2,
            "error": 17.870668411254883
        },
        "model.layers.20.self_attn.v_proj": {
            "bit_width": 3,
            "error": 20.38322639465332
        },
        "model.layers.20.self_attn.o_proj": {
            "bit_width": 3,
            "error": 20.24662208557129
        },
        "model.layers.20.mlp.gate_proj": {
            "bit_width": 3,
            "error": 21.88482666015625
        },
        "model.layers.20.mlp.up_proj": {
            "bit_width": 3,
            "error": 20.59280776977539
        },
        "model.layers.20.mlp.down_proj": {
            "bit_width": 3,
            "error": 20.01185417175293
        },
        "model.layers.21.self_attn.q_proj": {
            "bit_width": 2,
            "error": 16.643177032470703
        },
        "model.layers.21.self_attn.k_proj": {
            "bit_width": 2,
            "error": 17.3566837310791
        },
        "model.layers.21.self_attn.v_proj": {
            "bit_width": 3,
            "error": 19.54522132873535
        },
        "model.layers.21.self_attn.o_proj": {
            "bit_width": 3,
            "error": 19.87062644958496
        },
        "model.layers.21.mlp.gate_proj": {
            "bit_width": 3,
            "error": 22.050403594970703
        },
        "model.layers.21.mlp.up_proj": {
            "bit_width": 3,
            "error": 20.884567260742188
        },
        "model.layers.21.mlp.down_proj": {
            "bit_width": 3,
            "error": 20.264888763427734
        },
        "model.layers.22.self_attn.q_proj": {
            "bit_width": 2,
            "error": 17.136219024658203
        },
        "model.layers.22.self_attn.k_proj": {
            "bit_width": 2,
            "error": 16.5582275390625
        },
        "model.layers.22.self_attn.v_proj": {
            "bit_width": 3,
            "error": 20.268970489501953
        },
        "model.layers.22.self_attn.o_proj": {
            "bit_width": 2,
            "error": 16.62485694885254
        },
        "model.layers.22.mlp.gate_proj": {
            "bit_width": 3,
            "error": 21.3653564453125
        },
        "model.layers.22.mlp.up_proj": {
            "bit_width": 3,
            "error": 20.401323318481445
        },
        "model.layers.22.mlp.down_proj": {
            "bit_width": 3,
            "error": 20.23668670654297
        },
        "model.layers.23.self_attn.q_proj": {
            "bit_width": 2,
            "error": 16.795574188232422
        },
        "model.layers.23.self_attn.k_proj": {
            "bit_width": 2,
            "error": 17.393796920776367
        },
        "model.layers.23.self_attn.v_proj": {
            "bit_width": 3,
            "error": 19.613248825073242
        },
        "model.layers.23.self_attn.o_proj": {
            "bit_width": 2,
            "error": 16.655933380126953
        },
        "model.layers.23.mlp.gate_proj": {
            "bit_width": 3,
            "error": 21.228952407836914
        },
        "model.layers.23.mlp.up_proj": {
            "bit_width": 3,
            "error": 20.143396377563477
        },
        "model.layers.23.mlp.down_proj": {
            "bit_width": 3,
            "error": 19.983505249023438
        },
        "model.layers.24.self_attn.q_proj": {
            "bit_width": 2,
            "error": 16.974151611328125
        },
        "model.layers.24.self_attn.k_proj": {
            "bit_width": 2,
            "error": 17.52920913696289
        },
        "model.layers.24.self_attn.v_proj": {
            "bit_width": 3,
            "error": 19.586376190185547
        },
        "model.layers.24.self_attn.o_proj": {
            "bit_width": 2,
            "error": 15.25994873046875
        },
        "model.layers.24.mlp.gate_proj": {
            "bit_width": 3,
            "error": 20.942575454711914
        },
        "model.layers.24.mlp.up_proj": {
            "bit_width": 3,
            "error": 20.371212005615234
        },
        "model.layers.24.mlp.down_proj": {
            "bit_width": 3,
            "error": 19.905790328979492
        },
        "model.layers.25.self_attn.q_proj": {
            "bit_width": 2,
            "error": 16.221065521240234
        },
        "model.layers.25.self_attn.k_proj": {
            "bit_width": 2,
            "error": 17.143054962158203
        },
        "model.layers.25.self_attn.v_proj": {
            "bit_width": 3,
            "error": 18.761260986328125
        },
        "model.layers.25.self_attn.o_proj": {
            "bit_width": 2,
            "error": 17.029804229736328
        },
        "model.layers.25.mlp.gate_proj": {
            "bit_width": 3,
            "error": 20.790916442871094
        },
        "model.layers.25.mlp.up_proj": {
            "bit_width": 3,
            "error": 20.233240127563477
        },
        "model.layers.25.mlp.down_proj": {
            "bit_width": 3,
            "error": 19.782297134399414
        },
        "model.layers.26.self_attn.q_proj": {
            "bit_width": 2,
            "error": 16.14051628112793
        },
        "model.layers.26.self_attn.k_proj": {
            "bit_width": 2,
            "error": 16.610942840576172
        },
        "model.layers.26.self_attn.v_proj": {
            "bit_width": 3,
            "error": 18.742860794067383
        },
        "model.layers.26.self_attn.o_proj": {
            "bit_width": 2,
            "error": 20.26169204711914
        },
        "model.layers.26.mlp.gate_proj": {
            "bit_width": 3,
            "error": 20.77333641052246
        },
        "model.layers.26.mlp.up_proj": {
            "bit_width": 3,
            "error": 20.252216339111328
        },
        "model.layers.26.mlp.down_proj": {
            "bit_width": 3,
            "error": 20.460460662841797
        },
        "model.layers.27.self_attn.q_proj": {
            "bit_width": 2,
            "error": 16.786998748779297
        },
        "model.layers.27.self_attn.k_proj": {
            "bit_width": 2,
            "error": 16.952781677246094
        },
        "model.layers.27.self_attn.v_proj": {
            "bit_width": 3,
            "error": 19.418642044067383
        },
        "model.layers.27.self_attn.o_proj": {
            "bit_width": 2,
            "error": 16.057531356811523
        },
        "model.layers.27.mlp.gate_proj": {
            "bit_width": 3,
            "error": 20.74091911315918
        },
        "model.layers.27.mlp.up_proj": {
            "bit_width": 3,
            "error": 20.32341957092285
        },
        "model.layers.27.mlp.down_proj": {
            "bit_width": 3,
            "error": 19.86701202392578
        },
        "model.layers.28.self_attn.q_proj": {
            "bit_width": 2,
            "error": 16.61001968383789
        },
        "model.layers.28.self_attn.k_proj": {
            "bit_width": 2,
            "error": 16.544879913330078
        },
        "model.layers.28.self_attn.v_proj": {
            "bit_width": 3,
            "error": 19.469406127929688
        },
        "model.layers.28.self_attn.o_proj": {
            "bit_width": 3,
            "error": 21.87088966369629
        },
        "model.layers.28.mlp.gate_proj": {
            "bit_width": 2,
            "error": 15.909849166870117
        },
        "model.layers.28.mlp.up_proj": {
            "bit_width": 3,
            "error": 21.303871154785156
        },
        "model.layers.28.mlp.down_proj": {
            "bit_width": 2,
            "error": 26.742706298828125
        },
        "model.layers.29.self_attn.q_proj": {
            "bit_width": 2,
            "error": 15.619173049926758
        },
        "model.layers.29.self_attn.k_proj": {
            "bit_width": 2,
            "error": 15.756002426147461
        },
        "model.layers.29.self_attn.v_proj": {
            "bit_width": 3,
            "error": 18.92125701904297
        },
        "model.layers.29.self_attn.o_proj": {
            "bit_width": 3,
            "error": 22.127376556396484
        },
        "model.layers.29.mlp.gate_proj": {
            "bit_width": 2,
            "error": 17.82900047302246
        },
        "model.layers.29.mlp.up_proj": {
            "bit_width": 2,
            "error": 15.226241111755371
        },
        "model.layers.29.mlp.down_proj": {
            "bit_width": 2,
            "error": 17.811079025268555
        },
        "lm_head": {
            "bit_width": 2,
            "error": 16.288646697998047
        }
    },
    "average_bit_width": 2.6350710900473935,
    "error_threshold": 15,
    "min_quantile": 0.1,
    "max_quantile": 0.9,
    "quantized_model_benchmarks": {
        "wikitext_accuracy": 0.3937812567949554,
        "mmlu_results": {
            "overall_score": 0.2822085889570552,
            "task_scores": "{\"Task\":{\"0\":\"business_ethics\",\"1\":\"medical_genetics\",\"2\":\"formal_logic\"},\"Score\":{\"0\":0.27,\"1\":0.3,\"2\":0.2777777778}}"
        },
        "sanity_check_string": "user\nTell a short story of humanity with happy ending\nsystem\n"
    }
}