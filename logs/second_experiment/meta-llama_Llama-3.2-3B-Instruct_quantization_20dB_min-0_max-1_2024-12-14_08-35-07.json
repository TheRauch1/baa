{
    "model_name": "meta-llama/Llama-3.2-3B-Instruct",
    "layerwise_quantization_info": {
        "model.layers.0.self_attn.q_proj": {
            "bit_width": 4,
            "error": 20.959537506103516
        },
        "model.layers.0.self_attn.k_proj": {
            "bit_width": 4,
            "error": 22.05679702758789
        },
        "model.layers.0.self_attn.v_proj": {
            "bit_width": 5,
            "error": 23.048582077026367
        },
        "model.layers.0.self_attn.o_proj": {
            "bit_width": 6,
            "error": 24.474153518676758
        },
        "model.layers.0.mlp.gate_proj": {
            "bit_width": 5,
            "error": 21.770593643188477
        },
        "model.layers.0.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.28179168701172
        },
        "model.layers.0.mlp.down_proj": {
            "bit_width": 6,
            "error": 24.30658721923828
        },
        "model.layers.1.self_attn.q_proj": {
            "bit_width": 4,
            "error": 24.840938568115234
        },
        "model.layers.1.self_attn.k_proj": {
            "bit_width": 4,
            "error": 26.076128005981445
        },
        "model.layers.1.self_attn.v_proj": {
            "bit_width": 5,
            "error": 24.101221084594727
        },
        "model.layers.1.self_attn.o_proj": {
            "bit_width": 6,
            "error": 23.64252281188965
        },
        "model.layers.1.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.98781394958496
        },
        "model.layers.1.mlp.up_proj": {
            "bit_width": 5,
            "error": 20.54020881652832
        },
        "model.layers.1.mlp.down_proj": {
            "bit_width": 8,
            "error": 21.493558883666992
        },
        "model.layers.2.self_attn.q_proj": {
            "bit_width": 5,
            "error": 25.432662963867188
        },
        "model.layers.2.self_attn.k_proj": {
            "bit_width": 5,
            "error": 26.24561309814453
        },
        "model.layers.2.self_attn.v_proj": {
            "bit_width": 5,
            "error": 23.336809158325195
        },
        "model.layers.2.self_attn.o_proj": {
            "bit_width": 6,
            "error": 24.842876434326172
        },
        "model.layers.2.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.700159072875977
        },
        "model.layers.2.mlp.up_proj": {
            "bit_width": 5,
            "error": 21.290494918823242
        },
        "model.layers.2.mlp.down_proj": {
            "bit_width": 6,
            "error": 24.58135414123535
        },
        "model.layers.3.self_attn.q_proj": {
            "bit_width": 4,
            "error": 23.968732833862305
        },
        "model.layers.3.self_attn.k_proj": {
            "bit_width": 4,
            "error": 24.651100158691406
        },
        "model.layers.3.self_attn.v_proj": {
            "bit_width": 5,
            "error": 23.865032196044922
        },
        "model.layers.3.self_attn.o_proj": {
            "bit_width": 5,
            "error": 20.464975357055664
        },
        "model.layers.3.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.510950088500977
        },
        "model.layers.3.mlp.up_proj": {
            "bit_width": 5,
            "error": 21.723098754882812
        },
        "model.layers.3.mlp.down_proj": {
            "bit_width": 5,
            "error": 21.905309677124023
        },
        "model.layers.4.self_attn.q_proj": {
            "bit_width": 4,
            "error": 21.55550193786621
        },
        "model.layers.4.self_attn.k_proj": {
            "bit_width": 4,
            "error": 23.713031768798828
        },
        "model.layers.4.self_attn.v_proj": {
            "bit_width": 5,
            "error": 23.891611099243164
        },
        "model.layers.4.self_attn.o_proj": {
            "bit_width": 6,
            "error": 26.196334838867188
        },
        "model.layers.4.mlp.gate_proj": {
            "bit_width": 5,
            "error": 24.509769439697266
        },
        "model.layers.4.mlp.up_proj": {
            "bit_width": 5,
            "error": 21.110153198242188
        },
        "model.layers.4.mlp.down_proj": {
            "bit_width": 5,
            "error": 22.50631332397461
        },
        "model.layers.5.self_attn.q_proj": {
            "bit_width": 4,
            "error": 21.296974182128906
        },
        "model.layers.5.self_attn.k_proj": {
            "bit_width": 4,
            "error": 20.43094253540039
        },
        "model.layers.5.self_attn.v_proj": {
            "bit_width": 5,
            "error": 24.075931549072266
        },
        "model.layers.5.self_attn.o_proj": {
            "bit_width": 6,
            "error": 25.830142974853516
        },
        "model.layers.5.mlp.gate_proj": {
            "bit_width": 5,
            "error": 25.042739868164062
        },
        "model.layers.5.mlp.up_proj": {
            "bit_width": 5,
            "error": 21.238439559936523
        },
        "model.layers.5.mlp.down_proj": {
            "bit_width": 5,
            "error": 20.933866500854492
        },
        "model.layers.6.self_attn.q_proj": {
            "bit_width": 4,
            "error": 21.777862548828125
        },
        "model.layers.6.self_attn.k_proj": {
            "bit_width": 4,
            "error": 24.071062088012695
        },
        "model.layers.6.self_attn.v_proj": {
            "bit_width": 5,
            "error": 25.01465606689453
        },
        "model.layers.6.self_attn.o_proj": {
            "bit_width": 6,
            "error": 26.115741729736328
        },
        "model.layers.6.mlp.gate_proj": {
            "bit_width": 5,
            "error": 25.171308517456055
        },
        "model.layers.6.mlp.up_proj": {
            "bit_width": 5,
            "error": 21.648433685302734
        },
        "model.layers.6.mlp.down_proj": {
            "bit_width": 5,
            "error": 21.90456771850586
        },
        "model.layers.7.self_attn.q_proj": {
            "bit_width": 4,
            "error": 23.493499755859375
        },
        "model.layers.7.self_attn.k_proj": {
            "bit_width": 4,
            "error": 25.490917205810547
        },
        "model.layers.7.self_attn.v_proj": {
            "bit_width": 5,
            "error": 25.518783569335938
        },
        "model.layers.7.self_attn.o_proj": {
            "bit_width": 6,
            "error": 25.9182071685791
        },
        "model.layers.7.mlp.gate_proj": {
            "bit_width": 5,
            "error": 25.292465209960938
        },
        "model.layers.7.mlp.up_proj": {
            "bit_width": 5,
            "error": 21.784936904907227
        },
        "model.layers.7.mlp.down_proj": {
            "bit_width": 5,
            "error": 21.446178436279297
        },
        "model.layers.8.self_attn.q_proj": {
            "bit_width": 4,
            "error": 20.258800506591797
        },
        "model.layers.8.self_attn.k_proj": {
            "bit_width": 4,
            "error": 22.451087951660156
        },
        "model.layers.8.self_attn.v_proj": {
            "bit_width": 5,
            "error": 24.70468521118164
        },
        "model.layers.8.self_attn.o_proj": {
            "bit_width": 6,
            "error": 24.86726951599121
        },
        "model.layers.8.mlp.gate_proj": {
            "bit_width": 5,
            "error": 25.21705436706543
        },
        "model.layers.8.mlp.up_proj": {
            "bit_width": 5,
            "error": 21.94431495666504
        },
        "model.layers.8.mlp.down_proj": {
            "bit_width": 5,
            "error": 21.291276931762695
        },
        "model.layers.9.self_attn.q_proj": {
            "bit_width": 4,
            "error": 21.444730758666992
        },
        "model.layers.9.self_attn.k_proj": {
            "bit_width": 4,
            "error": 22.812278747558594
        },
        "model.layers.9.self_attn.v_proj": {
            "bit_width": 5,
            "error": 24.301759719848633
        },
        "model.layers.9.self_attn.o_proj": {
            "bit_width": 6,
            "error": 25.15740966796875
        },
        "model.layers.9.mlp.gate_proj": {
            "bit_width": 5,
            "error": 24.671863555908203
        },
        "model.layers.9.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.245176315307617
        },
        "model.layers.9.mlp.down_proj": {
            "bit_width": 5,
            "error": 21.263931274414062
        },
        "model.layers.10.self_attn.q_proj": {
            "bit_width": 4,
            "error": 21.475439071655273
        },
        "model.layers.10.self_attn.k_proj": {
            "bit_width": 4,
            "error": 21.806528091430664
        },
        "model.layers.10.self_attn.v_proj": {
            "bit_width": 5,
            "error": 25.56583595275879
        },
        "model.layers.10.self_attn.o_proj": {
            "bit_width": 6,
            "error": 24.3869686126709
        },
        "model.layers.10.mlp.gate_proj": {
            "bit_width": 5,
            "error": 23.86742401123047
        },
        "model.layers.10.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.104278564453125
        },
        "model.layers.10.mlp.down_proj": {
            "bit_width": 5,
            "error": 21.331693649291992
        },
        "model.layers.11.self_attn.q_proj": {
            "bit_width": 4,
            "error": 23.436782836914062
        },
        "model.layers.11.self_attn.k_proj": {
            "bit_width": 4,
            "error": 24.846092224121094
        },
        "model.layers.11.self_attn.v_proj": {
            "bit_width": 5,
            "error": 25.224885940551758
        },
        "model.layers.11.self_attn.o_proj": {
            "bit_width": 6,
            "error": 25.083511352539062
        },
        "model.layers.11.mlp.gate_proj": {
            "bit_width": 5,
            "error": 23.31482696533203
        },
        "model.layers.11.mlp.up_proj": {
            "bit_width": 5,
            "error": 21.228851318359375
        },
        "model.layers.11.mlp.down_proj": {
            "bit_width": 5,
            "error": 20.366783142089844
        },
        "model.layers.12.self_attn.q_proj": {
            "bit_width": 5,
            "error": 26.1654109954834
        },
        "model.layers.12.self_attn.k_proj": {
            "bit_width": 4,
            "error": 20.992023468017578
        },
        "model.layers.12.self_attn.v_proj": {
            "bit_width": 5,
            "error": 24.587600708007812
        },
        "model.layers.12.self_attn.o_proj": {
            "bit_width": 6,
            "error": 23.39029312133789
        },
        "model.layers.12.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.38715934753418
        },
        "model.layers.12.mlp.up_proj": {
            "bit_width": 5,
            "error": 21.296142578125
        },
        "model.layers.12.mlp.down_proj": {
            "bit_width": 5,
            "error": 21.377723693847656
        },
        "model.layers.13.self_attn.q_proj": {
            "bit_width": 5,
            "error": 26.142162322998047
        },
        "model.layers.13.self_attn.k_proj": {
            "bit_width": 4,
            "error": 20.729907989501953
        },
        "model.layers.13.self_attn.v_proj": {
            "bit_width": 5,
            "error": 23.639450073242188
        },
        "model.layers.13.self_attn.o_proj": {
            "bit_width": 6,
            "error": 25.301496505737305
        },
        "model.layers.13.mlp.gate_proj": {
            "bit_width": 5,
            "error": 21.750925064086914
        },
        "model.layers.13.mlp.up_proj": {
            "bit_width": 5,
            "error": 21.2587833404541
        },
        "model.layers.13.mlp.down_proj": {
            "bit_width": 5,
            "error": 20.57275390625
        },
        "model.layers.14.self_attn.q_proj": {
            "bit_width": 5,
            "error": 25.624208450317383
        },
        "model.layers.14.self_attn.k_proj": {
            "bit_width": 4,
            "error": 21.930809020996094
        },
        "model.layers.14.self_attn.v_proj": {
            "bit_width": 5,
            "error": 23.736507415771484
        },
        "model.layers.14.self_attn.o_proj": {
            "bit_width": 6,
            "error": 25.500736236572266
        },
        "model.layers.14.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.011089324951172
        },
        "model.layers.14.mlp.up_proj": {
            "bit_width": 5,
            "error": 21.567852020263672
        },
        "model.layers.14.mlp.down_proj": {
            "bit_width": 5,
            "error": 21.647232055664062
        },
        "model.layers.15.self_attn.q_proj": {
            "bit_width": 4,
            "error": 20.214496612548828
        },
        "model.layers.15.self_attn.k_proj": {
            "bit_width": 4,
            "error": 20.349918365478516
        },
        "model.layers.15.self_attn.v_proj": {
            "bit_width": 5,
            "error": 23.920673370361328
        },
        "model.layers.15.self_attn.o_proj": {
            "bit_width": 6,
            "error": 25.703105926513672
        },
        "model.layers.15.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.745981216430664
        },
        "model.layers.15.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.13970947265625
        },
        "model.layers.15.mlp.down_proj": {
            "bit_width": 6,
            "error": 24.827985763549805
        },
        "model.layers.16.self_attn.q_proj": {
            "bit_width": 4,
            "error": 20.038774490356445
        },
        "model.layers.16.self_attn.k_proj": {
            "bit_width": 4,
            "error": 21.15805435180664
        },
        "model.layers.16.self_attn.v_proj": {
            "bit_width": 5,
            "error": 23.238826751708984
        },
        "model.layers.16.self_attn.o_proj": {
            "bit_width": 6,
            "error": 25.42569351196289
        },
        "model.layers.16.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.237329483032227
        },
        "model.layers.16.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.2535400390625
        },
        "model.layers.16.mlp.down_proj": {
            "bit_width": 5,
            "error": 20.519716262817383
        },
        "model.layers.17.self_attn.q_proj": {
            "bit_width": 4,
            "error": 20.167057037353516
        },
        "model.layers.17.self_attn.k_proj": {
            "bit_width": 4,
            "error": 20.684717178344727
        },
        "model.layers.17.self_attn.v_proj": {
            "bit_width": 5,
            "error": 23.936920166015625
        },
        "model.layers.17.self_attn.o_proj": {
            "bit_width": 6,
            "error": 26.07467269897461
        },
        "model.layers.17.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.241004943847656
        },
        "model.layers.17.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.320335388183594
        },
        "model.layers.17.mlp.down_proj": {
            "bit_width": 6,
            "error": 23.991477966308594
        },
        "model.layers.18.self_attn.q_proj": {
            "bit_width": 5,
            "error": 25.450403213500977
        },
        "model.layers.18.self_attn.k_proj": {
            "bit_width": 4,
            "error": 20.211193084716797
        },
        "model.layers.18.self_attn.v_proj": {
            "bit_width": 5,
            "error": 23.30919647216797
        },
        "model.layers.18.self_attn.o_proj": {
            "bit_width": 5,
            "error": 20.238759994506836
        },
        "model.layers.18.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.191904067993164
        },
        "model.layers.18.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.209718704223633
        },
        "model.layers.18.mlp.down_proj": {
            "bit_width": 5,
            "error": 22.185016632080078
        },
        "model.layers.19.self_attn.q_proj": {
            "bit_width": 4,
            "error": 20.642087936401367
        },
        "model.layers.19.self_attn.k_proj": {
            "bit_width": 4,
            "error": 20.993698120117188
        },
        "model.layers.19.self_attn.v_proj": {
            "bit_width": 5,
            "error": 22.993955612182617
        },
        "model.layers.19.self_attn.o_proj": {
            "bit_width": 6,
            "error": 24.001453399658203
        },
        "model.layers.19.mlp.gate_proj": {
            "bit_width": 5,
            "error": 21.802474975585938
        },
        "model.layers.19.mlp.up_proj": {
            "bit_width": 5,
            "error": 21.804391860961914
        },
        "model.layers.19.mlp.down_proj": {
            "bit_width": 5,
            "error": 21.345352172851562
        },
        "model.layers.20.self_attn.q_proj": {
            "bit_width": 4,
            "error": 20.641990661621094
        },
        "model.layers.20.self_attn.k_proj": {
            "bit_width": 4,
            "error": 20.18594741821289
        },
        "model.layers.20.self_attn.v_proj": {
            "bit_width": 5,
            "error": 23.44614028930664
        },
        "model.layers.20.self_attn.o_proj": {
            "bit_width": 5,
            "error": 21.28140640258789
        },
        "model.layers.20.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.282554626464844
        },
        "model.layers.20.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.42903709411621
        },
        "model.layers.20.mlp.down_proj": {
            "bit_width": 5,
            "error": 20.26876449584961
        },
        "model.layers.21.self_attn.q_proj": {
            "bit_width": 4,
            "error": 20.08464813232422
        },
        "model.layers.21.self_attn.k_proj": {
            "bit_width": 5,
            "error": 25.921525955200195
        },
        "model.layers.21.self_attn.v_proj": {
            "bit_width": 5,
            "error": 23.042083740234375
        },
        "model.layers.21.self_attn.o_proj": {
            "bit_width": 5,
            "error": 21.92066192626953
        },
        "model.layers.21.mlp.gate_proj": {
            "bit_width": 5,
            "error": 21.733821868896484
        },
        "model.layers.21.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.156208038330078
        },
        "model.layers.21.mlp.down_proj": {
            "bit_width": 5,
            "error": 22.179128646850586
        },
        "model.layers.22.self_attn.q_proj": {
            "bit_width": 4,
            "error": 21.20904541015625
        },
        "model.layers.22.self_attn.k_proj": {
            "bit_width": 4,
            "error": 20.130340576171875
        },
        "model.layers.22.self_attn.v_proj": {
            "bit_width": 5,
            "error": 23.308820724487305
        },
        "model.layers.22.self_attn.o_proj": {
            "bit_width": 5,
            "error": 20.323596954345703
        },
        "model.layers.22.mlp.gate_proj": {
            "bit_width": 5,
            "error": 21.92209243774414
        },
        "model.layers.22.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.3000545501709
        },
        "model.layers.22.mlp.down_proj": {
            "bit_width": 6,
            "error": 26.11193084716797
        },
        "model.layers.23.self_attn.q_proj": {
            "bit_width": 5,
            "error": 26.153514862060547
        },
        "model.layers.23.self_attn.k_proj": {
            "bit_width": 5,
            "error": 26.085487365722656
        },
        "model.layers.23.self_attn.v_proj": {
            "bit_width": 5,
            "error": 22.286073684692383
        },
        "model.layers.23.self_attn.o_proj": {
            "bit_width": 5,
            "error": 20.646503448486328
        },
        "model.layers.23.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.18015480041504
        },
        "model.layers.23.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.775150299072266
        },
        "model.layers.23.mlp.down_proj": {
            "bit_width": 5,
            "error": 21.051658630371094
        },
        "model.layers.24.self_attn.q_proj": {
            "bit_width": 4,
            "error": 20.178796768188477
        },
        "model.layers.24.self_attn.k_proj": {
            "bit_width": 5,
            "error": 26.021438598632812
        },
        "model.layers.24.self_attn.v_proj": {
            "bit_width": 5,
            "error": 22.42766571044922
        },
        "model.layers.24.self_attn.o_proj": {
            "bit_width": 5,
            "error": 21.78581428527832
        },
        "model.layers.24.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.925945281982422
        },
        "model.layers.24.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.892417907714844
        },
        "model.layers.24.mlp.down_proj": {
            "bit_width": 5,
            "error": 23.13043785095215
        },
        "model.layers.25.self_attn.q_proj": {
            "bit_width": 4,
            "error": 20.498023986816406
        },
        "model.layers.25.self_attn.k_proj": {
            "bit_width": 4,
            "error": 21.247737884521484
        },
        "model.layers.25.self_attn.v_proj": {
            "bit_width": 5,
            "error": 22.70217514038086
        },
        "model.layers.25.self_attn.o_proj": {
            "bit_width": 5,
            "error": 21.50466537475586
        },
        "model.layers.25.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.96238136291504
        },
        "model.layers.25.mlp.up_proj": {
            "bit_width": 5,
            "error": 23.551530838012695
        },
        "model.layers.25.mlp.down_proj": {
            "bit_width": 5,
            "error": 22.084217071533203
        },
        "model.layers.26.self_attn.q_proj": {
            "bit_width": 4,
            "error": 20.173377990722656
        },
        "model.layers.26.self_attn.k_proj": {
            "bit_width": 5,
            "error": 26.097612380981445
        },
        "model.layers.26.self_attn.v_proj": {
            "bit_width": 5,
            "error": 21.715530395507812
        },
        "model.layers.26.self_attn.o_proj": {
            "bit_width": 5,
            "error": 21.346553802490234
        },
        "model.layers.26.mlp.gate_proj": {
            "bit_width": 5,
            "error": 23.284225463867188
        },
        "model.layers.26.mlp.up_proj": {
            "bit_width": 5,
            "error": 25.038259506225586
        },
        "model.layers.26.mlp.down_proj": {
            "bit_width": 5,
            "error": 21.828189849853516
        },
        "model.layers.27.self_attn.q_proj": {
            "bit_width": 4,
            "error": 22.63292694091797
        },
        "model.layers.27.self_attn.k_proj": {
            "bit_width": 4,
            "error": 22.42107391357422
        },
        "model.layers.27.self_attn.v_proj": {
            "bit_width": 5,
            "error": 21.404781341552734
        },
        "model.layers.27.self_attn.o_proj": {
            "bit_width": 6,
            "error": 22.469881057739258
        },
        "model.layers.27.mlp.gate_proj": {
            "bit_width": 5,
            "error": 21.523929595947266
        },
        "model.layers.27.mlp.up_proj": {
            "bit_width": 5,
            "error": 23.306621551513672
        },
        "model.layers.27.mlp.down_proj": {
            "bit_width": 6,
            "error": 25.1857967376709
        },
        "lm_head": {
            "bit_width": 5,
            "error": 25.788652420043945
        }
    },
    "average_bit_width": 4.913705583756345,
    "error_threshold": 20,
    "min_quantile": 0,
    "max_quantile": 1,
    "quantized_model_benchmarks": {
        "wikitext_accuracy": 0.45489702730407233,
        "mmlu_results": {
            "overall_score": 0.41411042944785276,
            "task_scores": "{\"Task\":{\"0\":\"business_ethics\",\"1\":\"medical_genetics\",\"2\":\"formal_logic\"},\"Score\":{\"0\":0.45,\"1\":0.45,\"2\":0.3571428571}}"
        },
        "sanity_check_string": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 14 Dec 2024\n\nuser\n\nTell a short story of humanity with happy endingassistant\n\nOnce upon a time, in a small village nestled in the rolling hills of a far-off land, there lived a young girl named Sophia. Sophia was a kind and gentle soul, with a heart full of love for all living things. She spent her days tending to her family's garden, where she grew the most vibrant and colorful flowers anyone had ever seen.\n\nOne day, a severe storm swept through the village, destroying many homes and leaving the villagers in a state of despair"
    }
}