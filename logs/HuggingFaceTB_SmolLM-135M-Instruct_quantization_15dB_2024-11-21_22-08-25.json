{
    "model_name": "HuggingFaceTB/SmolLM-135M-Instruct",
    "original_model_accuracy": 0.39463618573207615,
    "layerwise_quantization_info": {
        "model.layers.0.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.148107528686523
        },
        "model.layers.0.self_attn.k_proj": {
            "bit_width": 4,
            "error": 21.05882453918457
        },
        "model.layers.0.self_attn.v_proj": {
            "bit_width": 4,
            "error": 15.896235466003418
        },
        "model.layers.0.self_attn.o_proj": {
            "bit_width": 6,
            "error": 23.33773422241211
        },
        "model.layers.0.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.70952033996582
        },
        "model.layers.0.mlp.up_proj": {
            "bit_width": 5,
            "error": 17.96436882019043
        },
        "model.layers.0.mlp.down_proj": {
            "bit_width": 4,
            "error": 18.24686050415039
        },
        "model.layers.1.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.853694915771484
        },
        "model.layers.1.self_attn.k_proj": {
            "bit_width": 4,
            "error": 16.429912567138672
        },
        "model.layers.1.self_attn.v_proj": {
            "bit_width": 4,
            "error": 18.05805015563965
        },
        "model.layers.1.self_attn.o_proj": {
            "bit_width": 4,
            "error": 15.541316986083984
        },
        "model.layers.1.mlp.gate_proj": {
            "bit_width": 4,
            "error": 17.16410255432129
        },
        "model.layers.1.mlp.up_proj": {
            "bit_width": 4,
            "error": 17.3346004486084
        },
        "model.layers.1.mlp.down_proj": {
            "bit_width": 5,
            "error": 20.548595428466797
        },
        "model.layers.2.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.15952491760254
        },
        "model.layers.2.self_attn.k_proj": {
            "bit_width": 5,
            "error": 20.64289093017578
        },
        "model.layers.2.self_attn.v_proj": {
            "bit_width": 4,
            "error": 18.042179107666016
        },
        "model.layers.2.self_attn.o_proj": {
            "bit_width": 4,
            "error": 15.956000328063965
        },
        "model.layers.2.mlp.gate_proj": {
            "bit_width": 4,
            "error": 17.485010147094727
        },
        "model.layers.2.mlp.up_proj": {
            "bit_width": 4,
            "error": 15.498408317565918
        },
        "model.layers.2.mlp.down_proj": {
            "bit_width": 6,
            "error": 19.946247100830078
        },
        "model.layers.3.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.185009002685547
        },
        "model.layers.3.self_attn.k_proj": {
            "bit_width": 4,
            "error": 16.92548942565918
        },
        "model.layers.3.self_attn.v_proj": {
            "bit_width": 4,
            "error": 18.79532241821289
        },
        "model.layers.3.self_attn.o_proj": {
            "bit_width": 4,
            "error": 17.015169143676758
        },
        "model.layers.3.mlp.gate_proj": {
            "bit_width": 4,
            "error": 17.618207931518555
        },
        "model.layers.3.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.71816635131836
        },
        "model.layers.3.mlp.down_proj": {
            "bit_width": 5,
            "error": 19.102819442749023
        },
        "model.layers.4.self_attn.q_proj": {
            "bit_width": 4,
            "error": 15.850968360900879
        },
        "model.layers.4.self_attn.k_proj": {
            "bit_width": 4,
            "error": 18.12147331237793
        },
        "model.layers.4.self_attn.v_proj": {
            "bit_width": 4,
            "error": 19.0640811920166
        },
        "model.layers.4.self_attn.o_proj": {
            "bit_width": 4,
            "error": 17.407751083374023
        },
        "model.layers.4.mlp.gate_proj": {
            "bit_width": 4,
            "error": 18.032777786254883
        },
        "model.layers.4.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.5989933013916
        },
        "model.layers.4.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.573772430419922
        },
        "model.layers.5.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.681568145751953
        },
        "model.layers.5.self_attn.k_proj": {
            "bit_width": 4,
            "error": 17.805078506469727
        },
        "model.layers.5.self_attn.v_proj": {
            "bit_width": 4,
            "error": 19.7051944732666
        },
        "model.layers.5.self_attn.o_proj": {
            "bit_width": 4,
            "error": 17.175033569335938
        },
        "model.layers.5.mlp.gate_proj": {
            "bit_width": 4,
            "error": 18.669437408447266
        },
        "model.layers.5.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.69708824157715
        },
        "model.layers.5.mlp.down_proj": {
            "bit_width": 4,
            "error": 16.972097396850586
        },
        "model.layers.6.self_attn.q_proj": {
            "bit_width": 4,
            "error": 17.234079360961914
        },
        "model.layers.6.self_attn.k_proj": {
            "bit_width": 4,
            "error": 17.539688110351562
        },
        "model.layers.6.self_attn.v_proj": {
            "bit_width": 4,
            "error": 18.698087692260742
        },
        "model.layers.6.self_attn.o_proj": {
            "bit_width": 4,
            "error": 18.375341415405273
        },
        "model.layers.6.mlp.gate_proj": {
            "bit_width": 4,
            "error": 18.23822593688965
        },
        "model.layers.6.mlp.up_proj": {
            "bit_width": 4,
            "error": 17.04044532775879
        },
        "model.layers.6.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.854230880737305
        },
        "model.layers.7.self_attn.q_proj": {
            "bit_width": 4,
            "error": 15.476676940917969
        },
        "model.layers.7.self_attn.k_proj": {
            "bit_width": 4,
            "error": 17.277175903320312
        },
        "model.layers.7.self_attn.v_proj": {
            "bit_width": 4,
            "error": 19.255725860595703
        },
        "model.layers.7.self_attn.o_proj": {
            "bit_width": 4,
            "error": 17.479997634887695
        },
        "model.layers.7.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.839153289794922
        },
        "model.layers.7.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.490755081176758
        },
        "model.layers.7.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.131155014038086
        },
        "model.layers.8.self_attn.q_proj": {
            "bit_width": 4,
            "error": 18.155235290527344
        },
        "model.layers.8.self_attn.k_proj": {
            "bit_width": 4,
            "error": 19.671977996826172
        },
        "model.layers.8.self_attn.v_proj": {
            "bit_width": 4,
            "error": 18.121225357055664
        },
        "model.layers.8.self_attn.o_proj": {
            "bit_width": 4,
            "error": 17.513242721557617
        },
        "model.layers.8.mlp.gate_proj": {
            "bit_width": 4,
            "error": 18.02727508544922
        },
        "model.layers.8.mlp.up_proj": {
            "bit_width": 4,
            "error": 15.097184181213379
        },
        "model.layers.8.mlp.down_proj": {
            "bit_width": 4,
            "error": 16.901582717895508
        },
        "model.layers.9.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.591100692749023
        },
        "model.layers.9.self_attn.k_proj": {
            "bit_width": 4,
            "error": 17.089365005493164
        },
        "model.layers.9.self_attn.v_proj": {
            "bit_width": 4,
            "error": 18.22059440612793
        },
        "model.layers.9.self_attn.o_proj": {
            "bit_width": 4,
            "error": 15.566219329833984
        },
        "model.layers.9.mlp.gate_proj": {
            "bit_width": 4,
            "error": 17.907596588134766
        },
        "model.layers.9.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.54261016845703
        },
        "model.layers.9.mlp.down_proj": {
            "bit_width": 4,
            "error": 16.697938919067383
        },
        "model.layers.10.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.95209503173828
        },
        "model.layers.10.self_attn.k_proj": {
            "bit_width": 4,
            "error": 18.3114070892334
        },
        "model.layers.10.self_attn.v_proj": {
            "bit_width": 4,
            "error": 19.063261032104492
        },
        "model.layers.10.self_attn.o_proj": {
            "bit_width": 4,
            "error": 18.534727096557617
        },
        "model.layers.10.mlp.gate_proj": {
            "bit_width": 4,
            "error": 18.510473251342773
        },
        "model.layers.10.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.45423698425293
        },
        "model.layers.10.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.524860382080078
        },
        "model.layers.11.self_attn.q_proj": {
            "bit_width": 4,
            "error": 15.317264556884766
        },
        "model.layers.11.self_attn.k_proj": {
            "bit_width": 4,
            "error": 18.710721969604492
        },
        "model.layers.11.self_attn.v_proj": {
            "bit_width": 4,
            "error": 19.062599182128906
        },
        "model.layers.11.self_attn.o_proj": {
            "bit_width": 4,
            "error": 18.4143009185791
        },
        "model.layers.11.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.08614158630371
        },
        "model.layers.11.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.004642486572266
        },
        "model.layers.11.mlp.down_proj": {
            "bit_width": 5,
            "error": 18.532079696655273
        },
        "model.layers.12.self_attn.q_proj": {
            "bit_width": 5,
            "error": 20.48552703857422
        },
        "model.layers.12.self_attn.k_proj": {
            "bit_width": 4,
            "error": 16.337671279907227
        },
        "model.layers.12.self_attn.v_proj": {
            "bit_width": 4,
            "error": 18.798229217529297
        },
        "model.layers.12.self_attn.o_proj": {
            "bit_width": 4,
            "error": 16.77353858947754
        },
        "model.layers.12.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.701505661010742
        },
        "model.layers.12.mlp.up_proj": {
            "bit_width": 4,
            "error": 15.337667465209961
        },
        "model.layers.12.mlp.down_proj": {
            "bit_width": 6,
            "error": 21.17026138305664
        },
        "model.layers.13.self_attn.q_proj": {
            "bit_width": 5,
            "error": 19.780193328857422
        },
        "model.layers.13.self_attn.k_proj": {
            "bit_width": 5,
            "error": 21.629783630371094
        },
        "model.layers.13.self_attn.v_proj": {
            "bit_width": 4,
            "error": 19.339487075805664
        },
        "model.layers.13.self_attn.o_proj": {
            "bit_width": 4,
            "error": 18.94472312927246
        },
        "model.layers.13.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.441457748413086
        },
        "model.layers.13.mlp.up_proj": {
            "bit_width": 4,
            "error": 15.895275115966797
        },
        "model.layers.13.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.133262634277344
        },
        "model.layers.14.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.91804313659668
        },
        "model.layers.14.self_attn.k_proj": {
            "bit_width": 4,
            "error": 18.506574630737305
        },
        "model.layers.14.self_attn.v_proj": {
            "bit_width": 4,
            "error": 17.208389282226562
        },
        "model.layers.14.self_attn.o_proj": {
            "bit_width": 4,
            "error": 17.385019302368164
        },
        "model.layers.14.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.660497665405273
        },
        "model.layers.14.mlp.up_proj": {
            "bit_width": 5,
            "error": 20.132823944091797
        },
        "model.layers.14.mlp.down_proj": {
            "bit_width": 4,
            "error": 16.732242584228516
        },
        "model.layers.15.self_attn.q_proj": {
            "bit_width": 4,
            "error": 15.913386344909668
        },
        "model.layers.15.self_attn.k_proj": {
            "bit_width": 5,
            "error": 19.65285873413086
        },
        "model.layers.15.self_attn.v_proj": {
            "bit_width": 4,
            "error": 19.61391258239746
        },
        "model.layers.15.self_attn.o_proj": {
            "bit_width": 4,
            "error": 15.659652709960938
        },
        "model.layers.15.mlp.gate_proj": {
            "bit_width": 4,
            "error": 15.811834335327148
        },
        "model.layers.15.mlp.up_proj": {
            "bit_width": 4,
            "error": 15.806205749511719
        },
        "model.layers.15.mlp.down_proj": {
            "bit_width": 4,
            "error": 16.95124053955078
        },
        "model.layers.16.self_attn.q_proj": {
            "bit_width": 5,
            "error": 20.225502014160156
        },
        "model.layers.16.self_attn.k_proj": {
            "bit_width": 5,
            "error": 19.636598587036133
        },
        "model.layers.16.self_attn.v_proj": {
            "bit_width": 4,
            "error": 19.22349739074707
        },
        "model.layers.16.self_attn.o_proj": {
            "bit_width": 4,
            "error": 18.562713623046875
        },
        "model.layers.16.mlp.gate_proj": {
            "bit_width": 4,
            "error": 15.800217628479004
        },
        "model.layers.16.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.071462631225586
        },
        "model.layers.16.mlp.down_proj": {
            "bit_width": 4,
            "error": 16.670833587646484
        },
        "model.layers.17.self_attn.q_proj": {
            "bit_width": 5,
            "error": 20.052736282348633
        },
        "model.layers.17.self_attn.k_proj": {
            "bit_width": 5,
            "error": 20.819190979003906
        },
        "model.layers.17.self_attn.v_proj": {
            "bit_width": 4,
            "error": 19.42950439453125
        },
        "model.layers.17.self_attn.o_proj": {
            "bit_width": 4,
            "error": 16.31470489501953
        },
        "model.layers.17.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.006013870239258
        },
        "model.layers.17.mlp.up_proj": {
            "bit_width": 4,
            "error": 15.974685668945312
        },
        "model.layers.17.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.121212005615234
        },
        "model.layers.18.self_attn.q_proj": {
            "bit_width": 5,
            "error": 20.026437759399414
        },
        "model.layers.18.self_attn.k_proj": {
            "bit_width": 5,
            "error": 20.72465705871582
        },
        "model.layers.18.self_attn.v_proj": {
            "bit_width": 4,
            "error": 17.74989128112793
        },
        "model.layers.18.self_attn.o_proj": {
            "bit_width": 4,
            "error": 17.818374633789062
        },
        "model.layers.18.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.047666549682617
        },
        "model.layers.18.mlp.up_proj": {
            "bit_width": 4,
            "error": 15.47005558013916
        },
        "model.layers.18.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.145015716552734
        },
        "model.layers.19.self_attn.q_proj": {
            "bit_width": 5,
            "error": 20.739803314208984
        },
        "model.layers.19.self_attn.k_proj": {
            "bit_width": 5,
            "error": 21.09003257751465
        },
        "model.layers.19.self_attn.v_proj": {
            "bit_width": 4,
            "error": 17.279600143432617
        },
        "model.layers.19.self_attn.o_proj": {
            "bit_width": 4,
            "error": 16.8040714263916
        },
        "model.layers.19.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.039236068725586
        },
        "model.layers.19.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.934614181518555
        },
        "model.layers.19.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.276105880737305
        },
        "model.layers.20.self_attn.q_proj": {
            "bit_width": 5,
            "error": 20.750944137573242
        },
        "model.layers.20.self_attn.k_proj": {
            "bit_width": 4,
            "error": 15.34691047668457
        },
        "model.layers.20.self_attn.v_proj": {
            "bit_width": 4,
            "error": 18.273784637451172
        },
        "model.layers.20.self_attn.o_proj": {
            "bit_width": 4,
            "error": 16.802289962768555
        },
        "model.layers.20.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.141145706176758
        },
        "model.layers.20.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.90022850036621
        },
        "model.layers.20.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.36046028137207
        },
        "model.layers.21.self_attn.q_proj": {
            "bit_width": 4,
            "error": 17.078001022338867
        },
        "model.layers.21.self_attn.k_proj": {
            "bit_width": 4,
            "error": 17.042421340942383
        },
        "model.layers.21.self_attn.v_proj": {
            "bit_width": 4,
            "error": 18.677791595458984
        },
        "model.layers.21.self_attn.o_proj": {
            "bit_width": 4,
            "error": 16.873754501342773
        },
        "model.layers.21.mlp.gate_proj": {
            "bit_width": 4,
            "error": 15.529766082763672
        },
        "model.layers.21.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.437658309936523
        },
        "model.layers.21.mlp.down_proj": {
            "bit_width": 4,
            "error": 16.721603393554688
        },
        "model.layers.22.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.553695678710938
        },
        "model.layers.22.self_attn.k_proj": {
            "bit_width": 4,
            "error": 17.729440689086914
        },
        "model.layers.22.self_attn.v_proj": {
            "bit_width": 4,
            "error": 17.91408920288086
        },
        "model.layers.22.self_attn.o_proj": {
            "bit_width": 4,
            "error": 17.602462768554688
        },
        "model.layers.22.mlp.gate_proj": {
            "bit_width": 4,
            "error": 15.90926742553711
        },
        "model.layers.22.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.512224197387695
        },
        "model.layers.22.mlp.down_proj": {
            "bit_width": 4,
            "error": 16.291391372680664
        },
        "model.layers.23.self_attn.q_proj": {
            "bit_width": 4,
            "error": 17.202648162841797
        },
        "model.layers.23.self_attn.k_proj": {
            "bit_width": 4,
            "error": 17.84540367126465
        },
        "model.layers.23.self_attn.v_proj": {
            "bit_width": 4,
            "error": 17.58359146118164
        },
        "model.layers.23.self_attn.o_proj": {
            "bit_width": 4,
            "error": 19.193143844604492
        },
        "model.layers.23.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.131193161010742
        },
        "model.layers.23.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.88465118408203
        },
        "model.layers.23.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.03199577331543
        },
        "model.layers.24.self_attn.q_proj": {
            "bit_width": 4,
            "error": 17.22294044494629
        },
        "model.layers.24.self_attn.k_proj": {
            "bit_width": 4,
            "error": 16.931087493896484
        },
        "model.layers.24.self_attn.v_proj": {
            "bit_width": 4,
            "error": 18.22591781616211
        },
        "model.layers.24.self_attn.o_proj": {
            "bit_width": 4,
            "error": 17.580760955810547
        },
        "model.layers.24.mlp.gate_proj": {
            "bit_width": 4,
            "error": 15.825931549072266
        },
        "model.layers.24.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.049644470214844
        },
        "model.layers.24.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.870141983032227
        },
        "model.layers.25.self_attn.q_proj": {
            "bit_width": 4,
            "error": 17.130592346191406
        },
        "model.layers.25.self_attn.k_proj": {
            "bit_width": 4,
            "error": 18.58224105834961
        },
        "model.layers.25.self_attn.v_proj": {
            "bit_width": 4,
            "error": 18.173919677734375
        },
        "model.layers.25.self_attn.o_proj": {
            "bit_width": 4,
            "error": 19.617605209350586
        },
        "model.layers.25.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.1009578704834
        },
        "model.layers.25.mlp.up_proj": {
            "bit_width": 4,
            "error": 17.3361759185791
        },
        "model.layers.25.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.624500274658203
        },
        "model.layers.26.self_attn.q_proj": {
            "bit_width": 4,
            "error": 17.593360900878906
        },
        "model.layers.26.self_attn.k_proj": {
            "bit_width": 4,
            "error": 18.15700340270996
        },
        "model.layers.26.self_attn.v_proj": {
            "bit_width": 4,
            "error": 18.329437255859375
        },
        "model.layers.26.self_attn.o_proj": {
            "bit_width": 4,
            "error": 21.247364044189453
        },
        "model.layers.26.mlp.gate_proj": {
            "bit_width": 4,
            "error": 15.575048446655273
        },
        "model.layers.26.mlp.up_proj": {
            "bit_width": 4,
            "error": 17.088394165039062
        },
        "model.layers.26.mlp.down_proj": {
            "bit_width": 5,
            "error": 19.871337890625
        },
        "model.layers.27.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.876018524169922
        },
        "model.layers.27.self_attn.k_proj": {
            "bit_width": 4,
            "error": 15.223355293273926
        },
        "model.layers.27.self_attn.v_proj": {
            "bit_width": 4,
            "error": 18.26573944091797
        },
        "model.layers.27.self_attn.o_proj": {
            "bit_width": 4,
            "error": 19.782737731933594
        },
        "model.layers.27.mlp.gate_proj": {
            "bit_width": 4,
            "error": 16.42797088623047
        },
        "model.layers.27.mlp.up_proj": {
            "bit_width": 4,
            "error": 16.942066192626953
        },
        "model.layers.27.mlp.down_proj": {
            "bit_width": 4,
            "error": 17.638652801513672
        },
        "model.layers.28.self_attn.q_proj": {
            "bit_width": 4,
            "error": 16.021617889404297
        },
        "model.layers.28.self_attn.k_proj": {
            "bit_width": 4,
            "error": 15.892183303833008
        },
        "model.layers.28.self_attn.v_proj": {
            "bit_width": 4,
            "error": 17.645374298095703
        },
        "model.layers.28.self_attn.o_proj": {
            "bit_width": 4,
            "error": 18.89298439025879
        },
        "model.layers.28.mlp.gate_proj": {
            "bit_width": 6,
            "error": 21.213626861572266
        },
        "model.layers.28.mlp.up_proj": {
            "bit_width": 5,
            "error": 17.376102447509766
        },
        "model.layers.28.mlp.down_proj": {
            "bit_width": 5,
            "error": 17.225013732910156
        },
        "model.layers.29.self_attn.q_proj": {
            "bit_width": 4,
            "error": 15.990640640258789
        },
        "model.layers.29.self_attn.k_proj": {
            "bit_width": 4,
            "error": 16.93091583251953
        },
        "model.layers.29.self_attn.v_proj": {
            "bit_width": 4,
            "error": 17.446704864501953
        },
        "model.layers.29.self_attn.o_proj": {
            "bit_width": 4,
            "error": 18.17522621154785
        },
        "model.layers.29.mlp.gate_proj": {
            "bit_width": 4,
            "error": 15.152351379394531
        },
        "model.layers.29.mlp.up_proj": {
            "bit_width": 5,
            "error": 16.831302642822266
        },
        "model.layers.29.mlp.down_proj": {
            "bit_width": 5,
            "error": 18.666109085083008
        },
        "lm_head": {
            "bit_width": 4,
            "error": 18.77729034423828
        }
    },
    "average_bit_width": 4.151658767772512,
    "error_threshold": 15,
    "min_quantile": 0.0,
    "max_quantile": 1.0
}