{
    "model_name": "HuggingFaceTB/SmolLM-135M-Instruct",
    "original_model_accuracy": 0.39463618573207615,
    "layerwise_quantization_info": {
        "model.layers.0.self_attn.q_proj": {
            "bit_width": 2,
            "error": 17.32345962524414
        },
        "model.layers.0.self_attn.k_proj": {
            "bit_width": 2,
            "error": 17.938058853149414
        },
        "model.layers.0.self_attn.v_proj": {
            "bit_width": 2,
            "error": 17.712610244750977
        },
        "model.layers.0.self_attn.o_proj": {
            "bit_width": 2,
            "error": 23.1667537689209
        },
        "model.layers.0.mlp.gate_proj": {
            "bit_width": 2,
            "error": 15.152921676635742
        },
        "model.layers.0.mlp.up_proj": {
            "bit_width": 3,
            "error": 19.079334259033203
        },
        "model.layers.0.mlp.down_proj": {
            "bit_width": 2,
            "error": 19.48478126525879
        },
        "model.layers.1.self_attn.q_proj": {
            "bit_width": 2,
            "error": 15.735249519348145
        },
        "model.layers.1.self_attn.k_proj": {
            "bit_width": 2,
            "error": 15.284473419189453
        },
        "model.layers.1.self_attn.v_proj": {
            "bit_width": 2,
            "error": 14.712831497192383
        },
        "model.layers.1.self_attn.o_proj": {
            "bit_width": 2,
            "error": 19.3974609375
        },
        "model.layers.1.mlp.gate_proj": {
            "bit_width": 2,
            "error": 14.088661193847656
        },
        "model.layers.1.mlp.up_proj": {
            "bit_width": 2,
            "error": 12.827228546142578
        },
        "model.layers.1.mlp.down_proj": {
            "bit_width": 2,
            "error": 16.012407302856445
        },
        "model.layers.2.self_attn.q_proj": {
            "bit_width": 2,
            "error": 15.2724609375
        },
        "model.layers.2.self_attn.k_proj": {
            "bit_width": 2,
            "error": 16.550975799560547
        },
        "model.layers.2.self_attn.v_proj": {
            "bit_width": 2,
            "error": 13.105584144592285
        },
        "model.layers.2.self_attn.o_proj": {
            "bit_width": 2,
            "error": 12.842745780944824
        },
        "model.layers.2.mlp.gate_proj": {
            "bit_width": 2,
            "error": 13.906665802001953
        },
        "model.layers.2.mlp.up_proj": {
            "bit_width": 2,
            "error": 12.516207695007324
        },
        "model.layers.2.mlp.down_proj": {
            "bit_width": 2,
            "error": 15.785345077514648
        },
        "model.layers.3.self_attn.q_proj": {
            "bit_width": 2,
            "error": 14.845964431762695
        },
        "model.layers.3.self_attn.k_proj": {
            "bit_width": 2,
            "error": 15.404069900512695
        },
        "model.layers.3.self_attn.v_proj": {
            "bit_width": 2,
            "error": 12.70889663696289
        },
        "model.layers.3.self_attn.o_proj": {
            "bit_width": 2,
            "error": 12.71384334564209
        },
        "model.layers.3.mlp.gate_proj": {
            "bit_width": 2,
            "error": 13.447051048278809
        },
        "model.layers.3.mlp.up_proj": {
            "bit_width": 2,
            "error": 12.303446769714355
        },
        "model.layers.3.mlp.down_proj": {
            "bit_width": 2,
            "error": 13.63979721069336
        },
        "model.layers.4.self_attn.q_proj": {
            "bit_width": 2,
            "error": 15.597819328308105
        },
        "model.layers.4.self_attn.k_proj": {
            "bit_width": 2,
            "error": 15.739401817321777
        },
        "model.layers.4.self_attn.v_proj": {
            "bit_width": 2,
            "error": 13.186416625976562
        },
        "model.layers.4.self_attn.o_proj": {
            "bit_width": 2,
            "error": 12.205761909484863
        },
        "model.layers.4.mlp.gate_proj": {
            "bit_width": 2,
            "error": 14.0018310546875
        },
        "model.layers.4.mlp.up_proj": {
            "bit_width": 2,
            "error": 12.305974006652832
        },
        "model.layers.4.mlp.down_proj": {
            "bit_width": 2,
            "error": 12.60937213897705
        },
        "model.layers.5.self_attn.q_proj": {
            "bit_width": 2,
            "error": 15.318233489990234
        },
        "model.layers.5.self_attn.k_proj": {
            "bit_width": 2,
            "error": 15.553513526916504
        },
        "model.layers.5.self_attn.v_proj": {
            "bit_width": 2,
            "error": 13.195719718933105
        },
        "model.layers.5.self_attn.o_proj": {
            "bit_width": 2,
            "error": 12.333636283874512
        },
        "model.layers.5.mlp.gate_proj": {
            "bit_width": 2,
            "error": 13.884775161743164
        },
        "model.layers.5.mlp.up_proj": {
            "bit_width": 2,
            "error": 12.440372467041016
        },
        "model.layers.5.mlp.down_proj": {
            "bit_width": 2,
            "error": 12.808600425720215
        },
        "model.layers.6.self_attn.q_proj": {
            "bit_width": 2,
            "error": 15.077323913574219
        },
        "model.layers.6.self_attn.k_proj": {
            "bit_width": 2,
            "error": 15.508430480957031
        },
        "model.layers.6.self_attn.v_proj": {
            "bit_width": 2,
            "error": 13.149250030517578
        },
        "model.layers.6.self_attn.o_proj": {
            "bit_width": 2,
            "error": 12.56121826171875
        },
        "model.layers.6.mlp.gate_proj": {
            "bit_width": 2,
            "error": 13.802413940429688
        },
        "model.layers.6.mlp.up_proj": {
            "bit_width": 2,
            "error": 12.47537612915039
        },
        "model.layers.6.mlp.down_proj": {
            "bit_width": 2,
            "error": 12.846817016601562
        },
        "model.layers.7.self_attn.q_proj": {
            "bit_width": 2,
            "error": 14.631728172302246
        },
        "model.layers.7.self_attn.k_proj": {
            "bit_width": 2,
            "error": 15.671019554138184
        },
        "model.layers.7.self_attn.v_proj": {
            "bit_width": 2,
            "error": 12.727418899536133
        },
        "model.layers.7.self_attn.o_proj": {
            "bit_width": 2,
            "error": 12.876504898071289
        },
        "model.layers.7.mlp.gate_proj": {
            "bit_width": 2,
            "error": 14.335700988769531
        },
        "model.layers.7.mlp.up_proj": {
            "bit_width": 2,
            "error": 12.810108184814453
        },
        "model.layers.7.mlp.down_proj": {
            "bit_width": 2,
            "error": 13.873311042785645
        },
        "model.layers.8.self_attn.q_proj": {
            "bit_width": 2,
            "error": 15.323219299316406
        },
        "model.layers.8.self_attn.k_proj": {
            "bit_width": 2,
            "error": 15.397696495056152
        },
        "model.layers.8.self_attn.v_proj": {
            "bit_width": 2,
            "error": 13.41648006439209
        },
        "model.layers.8.self_attn.o_proj": {
            "bit_width": 2,
            "error": 12.713018417358398
        },
        "model.layers.8.mlp.gate_proj": {
            "bit_width": 2,
            "error": 14.309331893920898
        },
        "model.layers.8.mlp.up_proj": {
            "bit_width": 2,
            "error": 12.710838317871094
        },
        "model.layers.8.mlp.down_proj": {
            "bit_width": 2,
            "error": 13.245891571044922
        },
        "model.layers.9.self_attn.q_proj": {
            "bit_width": 2,
            "error": 15.287490844726562
        },
        "model.layers.9.self_attn.k_proj": {
            "bit_width": 2,
            "error": 15.135969161987305
        },
        "model.layers.9.self_attn.v_proj": {
            "bit_width": 2,
            "error": 14.363510131835938
        },
        "model.layers.9.self_attn.o_proj": {
            "bit_width": 3,
            "error": 18.739112854003906
        },
        "model.layers.9.mlp.gate_proj": {
            "bit_width": 2,
            "error": 14.0032377243042
        },
        "model.layers.9.mlp.up_proj": {
            "bit_width": 2,
            "error": 12.715424537658691
        },
        "model.layers.9.mlp.down_proj": {
            "bit_width": 2,
            "error": 12.9847993850708
        },
        "model.layers.10.self_attn.q_proj": {
            "bit_width": 2,
            "error": 14.682079315185547
        },
        "model.layers.10.self_attn.k_proj": {
            "bit_width": 2,
            "error": 14.647956848144531
        },
        "model.layers.10.self_attn.v_proj": {
            "bit_width": 2,
            "error": 12.228045463562012
        },
        "model.layers.10.self_attn.o_proj": {
            "bit_width": 2,
            "error": 12.960463523864746
        },
        "model.layers.10.mlp.gate_proj": {
            "bit_width": 2,
            "error": 14.920130729675293
        },
        "model.layers.10.mlp.up_proj": {
            "bit_width": 2,
            "error": 12.76275634765625
        },
        "model.layers.10.mlp.down_proj": {
            "bit_width": 2,
            "error": 12.718144416809082
        },
        "model.layers.11.self_attn.q_proj": {
            "bit_width": 2,
            "error": 15.017044067382812
        },
        "model.layers.11.self_attn.k_proj": {
            "bit_width": 2,
            "error": 14.4319429397583
        },
        "model.layers.11.self_attn.v_proj": {
            "bit_width": 2,
            "error": 13.047515869140625
        },
        "model.layers.11.self_attn.o_proj": {
            "bit_width": 2,
            "error": 12.585463523864746
        },
        "model.layers.11.mlp.gate_proj": {
            "bit_width": 2,
            "error": 14.659647941589355
        },
        "model.layers.11.mlp.up_proj": {
            "bit_width": 2,
            "error": 12.998008728027344
        },
        "model.layers.11.mlp.down_proj": {
            "bit_width": 2,
            "error": 39.01986312866211
        },
        "model.layers.12.self_attn.q_proj": {
            "bit_width": 2,
            "error": 17.210918426513672
        },
        "model.layers.12.self_attn.k_proj": {
            "bit_width": 2,
            "error": 15.76682186126709
        },
        "model.layers.12.self_attn.v_proj": {
            "bit_width": 2,
            "error": 13.307799339294434
        },
        "model.layers.12.self_attn.o_proj": {
            "bit_width": 3,
            "error": 19.10889434814453
        },
        "model.layers.12.mlp.gate_proj": {
            "bit_width": 2,
            "error": 14.659915924072266
        },
        "model.layers.12.mlp.up_proj": {
            "bit_width": 2,
            "error": 14.133684158325195
        },
        "model.layers.12.mlp.down_proj": {
            "bit_width": 2,
            "error": 16.417016983032227
        },
        "model.layers.13.self_attn.q_proj": {
            "bit_width": 2,
            "error": 18.229089736938477
        },
        "model.layers.13.self_attn.k_proj": {
            "bit_width": 2,
            "error": 16.013538360595703
        },
        "model.layers.13.self_attn.v_proj": {
            "bit_width": 2,
            "error": 14.613880157470703
        },
        "model.layers.13.self_attn.o_proj": {
            "bit_width": 2,
            "error": 14.404296875
        },
        "model.layers.13.mlp.gate_proj": {
            "bit_width": 2,
            "error": 14.76844310760498
        },
        "model.layers.13.mlp.up_proj": {
            "bit_width": 2,
            "error": 13.255739212036133
        },
        "model.layers.13.mlp.down_proj": {
            "bit_width": 2,
            "error": 12.955888748168945
        },
        "model.layers.14.self_attn.q_proj": {
            "bit_width": 2,
            "error": 17.698772430419922
        },
        "model.layers.14.self_attn.k_proj": {
            "bit_width": 2,
            "error": 18.437793731689453
        },
        "model.layers.14.self_attn.v_proj": {
            "bit_width": 2,
            "error": 15.159698486328125
        },
        "model.layers.14.self_attn.o_proj": {
            "bit_width": 2,
            "error": 13.610838890075684
        },
        "model.layers.14.mlp.gate_proj": {
            "bit_width": 2,
            "error": 14.225310325622559
        },
        "model.layers.14.mlp.up_proj": {
            "bit_width": 2,
            "error": 13.23996353149414
        },
        "model.layers.14.mlp.down_proj": {
            "bit_width": 2,
            "error": 12.749028205871582
        },
        "model.layers.15.self_attn.q_proj": {
            "bit_width": 2,
            "error": 16.474721908569336
        },
        "model.layers.15.self_attn.k_proj": {
            "bit_width": 2,
            "error": 17.666336059570312
        },
        "model.layers.15.self_attn.v_proj": {
            "bit_width": 2,
            "error": 13.194685935974121
        },
        "model.layers.15.self_attn.o_proj": {
            "bit_width": 3,
            "error": 18.824546813964844
        },
        "model.layers.15.mlp.gate_proj": {
            "bit_width": 2,
            "error": 13.991352081298828
        },
        "model.layers.15.mlp.up_proj": {
            "bit_width": 2,
            "error": 13.117441177368164
        },
        "model.layers.15.mlp.down_proj": {
            "bit_width": 2,
            "error": 12.651905059814453
        },
        "model.layers.16.self_attn.q_proj": {
            "bit_width": 2,
            "error": 17.21902084350586
        },
        "model.layers.16.self_attn.k_proj": {
            "bit_width": 2,
            "error": 18.186355590820312
        },
        "model.layers.16.self_attn.v_proj": {
            "bit_width": 2,
            "error": 12.657903671264648
        },
        "model.layers.16.self_attn.o_proj": {
            "bit_width": 2,
            "error": 13.669585227966309
        },
        "model.layers.16.mlp.gate_proj": {
            "bit_width": 2,
            "error": 14.219783782958984
        },
        "model.layers.16.mlp.up_proj": {
            "bit_width": 2,
            "error": 13.12951946258545
        },
        "model.layers.16.mlp.down_proj": {
            "bit_width": 2,
            "error": 12.587324142456055
        },
        "model.layers.17.self_attn.q_proj": {
            "bit_width": 2,
            "error": 17.186750411987305
        },
        "model.layers.17.self_attn.k_proj": {
            "bit_width": 2,
            "error": 17.4946346282959
        },
        "model.layers.17.self_attn.v_proj": {
            "bit_width": 2,
            "error": 13.292346000671387
        },
        "model.layers.17.self_attn.o_proj": {
            "bit_width": 2,
            "error": 12.19088077545166
        },
        "model.layers.17.mlp.gate_proj": {
            "bit_width": 2,
            "error": 14.112373352050781
        },
        "model.layers.17.mlp.up_proj": {
            "bit_width": 2,
            "error": 13.169637680053711
        },
        "model.layers.17.mlp.down_proj": {
            "bit_width": 2,
            "error": 12.645759582519531
        },
        "model.layers.18.self_attn.q_proj": {
            "bit_width": 2,
            "error": 17.38441276550293
        },
        "model.layers.18.self_attn.k_proj": {
            "bit_width": 2,
            "error": 18.352127075195312
        },
        "model.layers.18.self_attn.v_proj": {
            "bit_width": 2,
            "error": 12.875301361083984
        },
        "model.layers.18.self_attn.o_proj": {
            "bit_width": 2,
            "error": 13.715961456298828
        },
        "model.layers.18.mlp.gate_proj": {
            "bit_width": 2,
            "error": 14.633516311645508
        },
        "model.layers.18.mlp.up_proj": {
            "bit_width": 2,
            "error": 13.131246566772461
        },
        "model.layers.18.mlp.down_proj": {
            "bit_width": 2,
            "error": 12.626384735107422
        },
        "model.layers.19.self_attn.q_proj": {
            "bit_width": 2,
            "error": 17.58155632019043
        },
        "model.layers.19.self_attn.k_proj": {
            "bit_width": 2,
            "error": 17.47887420654297
        },
        "model.layers.19.self_attn.v_proj": {
            "bit_width": 2,
            "error": 12.665458679199219
        },
        "model.layers.19.self_attn.o_proj": {
            "bit_width": 2,
            "error": 14.112842559814453
        },
        "model.layers.19.mlp.gate_proj": {
            "bit_width": 2,
            "error": 14.564623832702637
        },
        "model.layers.19.mlp.up_proj": {
            "bit_width": 2,
            "error": 13.19609260559082
        },
        "model.layers.19.mlp.down_proj": {
            "bit_width": 2,
            "error": 12.426155090332031
        },
        "model.layers.20.self_attn.q_proj": {
            "bit_width": 2,
            "error": 17.69259262084961
        },
        "model.layers.20.self_attn.k_proj": {
            "bit_width": 2,
            "error": 17.889074325561523
        },
        "model.layers.20.self_attn.v_proj": {
            "bit_width": 2,
            "error": 13.26431655883789
        },
        "model.layers.20.self_attn.o_proj": {
            "bit_width": 2,
            "error": 13.043386459350586
        },
        "model.layers.20.mlp.gate_proj": {
            "bit_width": 2,
            "error": 14.691263198852539
        },
        "model.layers.20.mlp.up_proj": {
            "bit_width": 2,
            "error": 13.208902359008789
        },
        "model.layers.20.mlp.down_proj": {
            "bit_width": 2,
            "error": 12.63314151763916
        },
        "model.layers.21.self_attn.q_proj": {
            "bit_width": 2,
            "error": 16.602998733520508
        },
        "model.layers.21.self_attn.k_proj": {
            "bit_width": 2,
            "error": 17.347496032714844
        },
        "model.layers.21.self_attn.v_proj": {
            "bit_width": 2,
            "error": 12.09974479675293
        },
        "model.layers.21.self_attn.o_proj": {
            "bit_width": 2,
            "error": 12.508277893066406
        },
        "model.layers.21.mlp.gate_proj": {
            "bit_width": 2,
            "error": 14.70678424835205
        },
        "model.layers.21.mlp.up_proj": {
            "bit_width": 2,
            "error": 13.637598991394043
        },
        "model.layers.21.mlp.down_proj": {
            "bit_width": 2,
            "error": 12.927613258361816
        },
        "model.layers.22.self_attn.q_proj": {
            "bit_width": 2,
            "error": 17.110584259033203
        },
        "model.layers.22.self_attn.k_proj": {
            "bit_width": 2,
            "error": 16.56241798400879
        },
        "model.layers.22.self_attn.v_proj": {
            "bit_width": 2,
            "error": 12.952828407287598
        },
        "model.layers.22.self_attn.o_proj": {
            "bit_width": 2,
            "error": 17.016647338867188
        },
        "model.layers.22.mlp.gate_proj": {
            "bit_width": 2,
            "error": 14.021062850952148
        },
        "model.layers.22.mlp.up_proj": {
            "bit_width": 2,
            "error": 13.136877059936523
        },
        "model.layers.22.mlp.down_proj": {
            "bit_width": 2,
            "error": 12.868088722229004
        },
        "model.layers.23.self_attn.q_proj": {
            "bit_width": 2,
            "error": 16.811824798583984
        },
        "model.layers.23.self_attn.k_proj": {
            "bit_width": 2,
            "error": 17.392921447753906
        },
        "model.layers.23.self_attn.v_proj": {
            "bit_width": 2,
            "error": 12.281479835510254
        },
        "model.layers.23.self_attn.o_proj": {
            "bit_width": 2,
            "error": 17.19183349609375
        },
        "model.layers.23.mlp.gate_proj": {
            "bit_width": 2,
            "error": 13.952255249023438
        },
        "model.layers.23.mlp.up_proj": {
            "bit_width": 2,
            "error": 12.899409294128418
        },
        "model.layers.23.mlp.down_proj": {
            "bit_width": 2,
            "error": 12.800863265991211
        },
        "model.layers.24.self_attn.q_proj": {
            "bit_width": 2,
            "error": 17.015277862548828
        },
        "model.layers.24.self_attn.k_proj": {
            "bit_width": 2,
            "error": 17.597801208496094
        },
        "model.layers.24.self_attn.v_proj": {
            "bit_width": 2,
            "error": 12.370070457458496
        },
        "model.layers.24.self_attn.o_proj": {
            "bit_width": 2,
            "error": 15.806266784667969
        },
        "model.layers.24.mlp.gate_proj": {
            "bit_width": 2,
            "error": 13.668074607849121
        },
        "model.layers.24.mlp.up_proj": {
            "bit_width": 2,
            "error": 13.060873985290527
        },
        "model.layers.24.mlp.down_proj": {
            "bit_width": 2,
            "error": 12.57465934753418
        },
        "model.layers.25.self_attn.q_proj": {
            "bit_width": 2,
            "error": 16.275461196899414
        },
        "model.layers.25.self_attn.k_proj": {
            "bit_width": 2,
            "error": 17.16754722595215
        },
        "model.layers.25.self_attn.v_proj": {
            "bit_width": 3,
            "error": 18.990598678588867
        },
        "model.layers.25.self_attn.o_proj": {
            "bit_width": 2,
            "error": 17.353710174560547
        },
        "model.layers.25.mlp.gate_proj": {
            "bit_width": 2,
            "error": 13.563453674316406
        },
        "model.layers.25.mlp.up_proj": {
            "bit_width": 2,
            "error": 12.885984420776367
        },
        "model.layers.25.mlp.down_proj": {
            "bit_width": 2,
            "error": 12.545104026794434
        },
        "model.layers.26.self_attn.q_proj": {
            "bit_width": 2,
            "error": 16.2020263671875
        },
        "model.layers.26.self_attn.k_proj": {
            "bit_width": 2,
            "error": 16.483234405517578
        },
        "model.layers.26.self_attn.v_proj": {
            "bit_width": 3,
            "error": 18.982595443725586
        },
        "model.layers.26.self_attn.o_proj": {
            "bit_width": 2,
            "error": 20.351144790649414
        },
        "model.layers.26.mlp.gate_proj": {
            "bit_width": 2,
            "error": 13.44759750366211
        },
        "model.layers.26.mlp.up_proj": {
            "bit_width": 2,
            "error": 13.049413681030273
        },
        "model.layers.26.mlp.down_proj": {
            "bit_width": 2,
            "error": 13.255134582519531
        },
        "model.layers.27.self_attn.q_proj": {
            "bit_width": 2,
            "error": 16.94123077392578
        },
        "model.layers.27.self_attn.k_proj": {
            "bit_width": 2,
            "error": 16.95517349243164
        },
        "model.layers.27.self_attn.v_proj": {
            "bit_width": 2,
            "error": 12.116493225097656
        },
        "model.layers.27.self_attn.o_proj": {
            "bit_width": 2,
            "error": 16.21004867553711
        },
        "model.layers.27.mlp.gate_proj": {
            "bit_width": 2,
            "error": 13.437433242797852
        },
        "model.layers.27.mlp.up_proj": {
            "bit_width": 2,
            "error": 12.972747802734375
        },
        "model.layers.27.mlp.down_proj": {
            "bit_width": 2,
            "error": 12.562461853027344
        },
        "model.layers.28.self_attn.q_proj": {
            "bit_width": 2,
            "error": 16.69304084777832
        },
        "model.layers.28.self_attn.k_proj": {
            "bit_width": 2,
            "error": 16.527915954589844
        },
        "model.layers.28.self_attn.v_proj": {
            "bit_width": 3,
            "error": 19.475322723388672
        },
        "model.layers.28.self_attn.o_proj": {
            "bit_width": 2,
            "error": 14.58922290802002
        },
        "model.layers.28.mlp.gate_proj": {
            "bit_width": 2,
            "error": 15.900927543640137
        },
        "model.layers.28.mlp.up_proj": {
            "bit_width": 2,
            "error": 13.872937202453613
        },
        "model.layers.28.mlp.down_proj": {
            "bit_width": 2,
            "error": 26.955717086791992
        },
        "model.layers.29.self_attn.q_proj": {
            "bit_width": 2,
            "error": 15.669122695922852
        },
        "model.layers.29.self_attn.k_proj": {
            "bit_width": 2,
            "error": 15.683868408203125
        },
        "model.layers.29.self_attn.v_proj": {
            "bit_width": 3,
            "error": 18.890512466430664
        },
        "model.layers.29.self_attn.o_proj": {
            "bit_width": 2,
            "error": 14.757148742675781
        },
        "model.layers.29.mlp.gate_proj": {
            "bit_width": 2,
            "error": 17.88218879699707
        },
        "model.layers.29.mlp.up_proj": {
            "bit_width": 2,
            "error": 15.257554054260254
        },
        "model.layers.29.mlp.down_proj": {
            "bit_width": 2,
            "error": 17.698423385620117
        },
        "lm_head": {
            "bit_width": 2,
            "error": 16.293315887451172
        }
    },
    "average_bit_width": 2.037914691943128,
    "error_threshold": 12,
    "min_quantile": 0.1,
    "max_quantile": 0.9,
    "quantized_model_accuracy": 0.3020814801636719
}