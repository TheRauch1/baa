{
    "model_name": "HuggingFaceTB/SmolLM-135M-Instruct",
    "original_model_accuracy": 0.39463618573207615,
    "layerwise_quantization_info": {
        "model.layers.0.self_attn.q_proj": {
            "bit_width": 5,
            "error": 22.96016502380371
        },
        "model.layers.0.self_attn.k_proj": {
            "bit_width": 4,
            "error": 21.061429977416992
        },
        "model.layers.0.self_attn.v_proj": {
            "bit_width": 5,
            "error": 22.332849502563477
        },
        "model.layers.0.self_attn.o_proj": {
            "bit_width": 6,
            "error": 23.337879180908203
        },
        "model.layers.0.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.990419387817383
        },
        "model.layers.0.mlp.up_proj": {
            "bit_width": 6,
            "error": 24.22878646850586
        },
        "model.layers.0.mlp.down_proj": {
            "bit_width": 5,
            "error": 24.723073959350586
        },
        "model.layers.1.self_attn.q_proj": {
            "bit_width": 5,
            "error": 23.390628814697266
        },
        "model.layers.1.self_attn.k_proj": {
            "bit_width": 5,
            "error": 23.210548400878906
        },
        "model.layers.1.self_attn.v_proj": {
            "bit_width": 5,
            "error": 24.229961395263672
        },
        "model.layers.1.self_attn.o_proj": {
            "bit_width": 5,
            "error": 22.751710891723633
        },
        "model.layers.1.mlp.gate_proj": {
            "bit_width": 5,
            "error": 23.515832901000977
        },
        "model.layers.1.mlp.up_proj": {
            "bit_width": 5,
            "error": 23.651582717895508
        },
        "model.layers.1.mlp.down_proj": {
            "bit_width": 5,
            "error": 20.54852294921875
        },
        "model.layers.2.self_attn.q_proj": {
            "bit_width": 5,
            "error": 22.437448501586914
        },
        "model.layers.2.self_attn.k_proj": {
            "bit_width": 5,
            "error": 20.642927169799805
        },
        "model.layers.2.self_attn.v_proj": {
            "bit_width": 5,
            "error": 24.37490463256836
        },
        "model.layers.2.self_attn.o_proj": {
            "bit_width": 5,
            "error": 22.294443130493164
        },
        "model.layers.2.mlp.gate_proj": {
            "bit_width": 5,
            "error": 23.70287322998047
        },
        "model.layers.2.mlp.up_proj": {
            "bit_width": 5,
            "error": 21.886369705200195
        },
        "model.layers.2.mlp.down_proj": {
            "bit_width": 8,
            "error": 34.08197784423828
        },
        "model.layers.3.self_attn.q_proj": {
            "bit_width": 5,
            "error": 22.4027042388916
        },
        "model.layers.3.self_attn.k_proj": {
            "bit_width": 5,
            "error": 23.733551025390625
        },
        "model.layers.3.self_attn.v_proj": {
            "bit_width": 5,
            "error": 25.065595626831055
        },
        "model.layers.3.self_attn.o_proj": {
            "bit_width": 5,
            "error": 23.208282470703125
        },
        "model.layers.3.mlp.gate_proj": {
            "bit_width": 5,
            "error": 23.934856414794922
        },
        "model.layers.3.mlp.up_proj": {
            "bit_width": 5,
            "error": 23.0080509185791
        },
        "model.layers.3.mlp.down_proj": {
            "bit_width": 6,
            "error": 25.292686462402344
        },
        "model.layers.4.self_attn.q_proj": {
            "bit_width": 5,
            "error": 21.885047912597656
        },
        "model.layers.4.self_attn.k_proj": {
            "bit_width": 5,
            "error": 24.861289978027344
        },
        "model.layers.4.self_attn.v_proj": {
            "bit_width": 5,
            "error": 25.469268798828125
        },
        "model.layers.4.self_attn.o_proj": {
            "bit_width": 5,
            "error": 23.666213989257812
        },
        "model.layers.4.mlp.gate_proj": {
            "bit_width": 5,
            "error": 24.451763153076172
        },
        "model.layers.4.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.950639724731445
        },
        "model.layers.4.mlp.down_proj": {
            "bit_width": 5,
            "error": 23.90982437133789
        },
        "model.layers.5.self_attn.q_proj": {
            "bit_width": 5,
            "error": 22.53775405883789
        },
        "model.layers.5.self_attn.k_proj": {
            "bit_width": 5,
            "error": 23.927738189697266
        },
        "model.layers.5.self_attn.v_proj": {
            "bit_width": 5,
            "error": 26.006378173828125
        },
        "model.layers.5.self_attn.o_proj": {
            "bit_width": 5,
            "error": 23.343528747558594
        },
        "model.layers.5.mlp.gate_proj": {
            "bit_width": 5,
            "error": 24.978609085083008
        },
        "model.layers.5.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.86206817626953
        },
        "model.layers.5.mlp.down_proj": {
            "bit_width": 5,
            "error": 23.274215698242188
        },
        "model.layers.6.self_attn.q_proj": {
            "bit_width": 5,
            "error": 23.382896423339844
        },
        "model.layers.6.self_attn.k_proj": {
            "bit_width": 5,
            "error": 23.95716094970703
        },
        "model.layers.6.self_attn.v_proj": {
            "bit_width": 5,
            "error": 24.99570083618164
        },
        "model.layers.6.self_attn.o_proj": {
            "bit_width": 5,
            "error": 24.56453514099121
        },
        "model.layers.6.mlp.gate_proj": {
            "bit_width": 5,
            "error": 24.526897430419922
        },
        "model.layers.6.mlp.up_proj": {
            "bit_width": 5,
            "error": 23.341144561767578
        },
        "model.layers.6.mlp.down_proj": {
            "bit_width": 5,
            "error": 24.14140510559082
        },
        "model.layers.7.self_attn.q_proj": {
            "bit_width": 5,
            "error": 21.902881622314453
        },
        "model.layers.7.self_attn.k_proj": {
            "bit_width": 5,
            "error": 23.279067993164062
        },
        "model.layers.7.self_attn.v_proj": {
            "bit_width": 5,
            "error": 25.591053009033203
        },
        "model.layers.7.self_attn.o_proj": {
            "bit_width": 5,
            "error": 23.720190048217773
        },
        "model.layers.7.mlp.gate_proj": {
            "bit_width": 5,
            "error": 23.240995407104492
        },
        "model.layers.7.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.767074584960938
        },
        "model.layers.7.mlp.down_proj": {
            "bit_width": 5,
            "error": 23.52971076965332
        },
        "model.layers.8.self_attn.q_proj": {
            "bit_width": 5,
            "error": 24.334287643432617
        },
        "model.layers.8.self_attn.k_proj": {
            "bit_width": 5,
            "error": 25.923240661621094
        },
        "model.layers.8.self_attn.v_proj": {
            "bit_width": 5,
            "error": 24.628889083862305
        },
        "model.layers.8.self_attn.o_proj": {
            "bit_width": 5,
            "error": 23.809173583984375
        },
        "model.layers.8.mlp.gate_proj": {
            "bit_width": 5,
            "error": 24.21924591064453
        },
        "model.layers.8.mlp.up_proj": {
            "bit_width": 5,
            "error": 21.185426712036133
        },
        "model.layers.8.mlp.down_proj": {
            "bit_width": 5,
            "error": 22.65968132019043
        },
        "model.layers.9.self_attn.q_proj": {
            "bit_width": 5,
            "error": 22.765111923217773
        },
        "model.layers.9.self_attn.k_proj": {
            "bit_width": 5,
            "error": 23.20733070373535
        },
        "model.layers.9.self_attn.v_proj": {
            "bit_width": 5,
            "error": 24.69740867614746
        },
        "model.layers.9.self_attn.o_proj": {
            "bit_width": 5,
            "error": 22.09629249572754
        },
        "model.layers.9.mlp.gate_proj": {
            "bit_width": 5,
            "error": 24.242277145385742
        },
        "model.layers.9.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.871925354003906
        },
        "model.layers.9.mlp.down_proj": {
            "bit_width": 5,
            "error": 22.97809410095215
        },
        "model.layers.10.self_attn.q_proj": {
            "bit_width": 5,
            "error": 23.021318435668945
        },
        "model.layers.10.self_attn.k_proj": {
            "bit_width": 5,
            "error": 24.370649337768555
        },
        "model.layers.10.self_attn.v_proj": {
            "bit_width": 5,
            "error": 25.284114837646484
        },
        "model.layers.10.self_attn.o_proj": {
            "bit_width": 5,
            "error": 24.713245391845703
        },
        "model.layers.10.mlp.gate_proj": {
            "bit_width": 5,
            "error": 24.895835876464844
        },
        "model.layers.10.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.795677185058594
        },
        "model.layers.10.mlp.down_proj": {
            "bit_width": 5,
            "error": 23.80470085144043
        },
        "model.layers.11.self_attn.q_proj": {
            "bit_width": 5,
            "error": 21.851213455200195
        },
        "model.layers.11.self_attn.k_proj": {
            "bit_width": 5,
            "error": 24.896345138549805
        },
        "model.layers.11.self_attn.v_proj": {
            "bit_width": 5,
            "error": 25.27082633972168
        },
        "model.layers.11.self_attn.o_proj": {
            "bit_width": 5,
            "error": 24.70383071899414
        },
        "model.layers.11.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.28293228149414
        },
        "model.layers.11.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.340028762817383
        },
        "model.layers.11.mlp.down_proj": {
            "bit_width": 6,
            "error": 23.8414363861084
        },
        "model.layers.12.self_attn.q_proj": {
            "bit_width": 5,
            "error": 20.48526382446289
        },
        "model.layers.12.self_attn.k_proj": {
            "bit_width": 5,
            "error": 22.72394371032715
        },
        "model.layers.12.self_attn.v_proj": {
            "bit_width": 5,
            "error": 25.092449188232422
        },
        "model.layers.12.self_attn.o_proj": {
            "bit_width": 5,
            "error": 23.076717376708984
        },
        "model.layers.12.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.974720001220703
        },
        "model.layers.12.mlp.up_proj": {
            "bit_width": 5,
            "error": 21.864072799682617
        },
        "model.layers.12.mlp.down_proj": {
            "bit_width": 6,
            "error": 21.170244216918945
        },
        "model.layers.13.self_attn.q_proj": {
            "bit_width": 6,
            "error": 26.250484466552734
        },
        "model.layers.13.self_attn.k_proj": {
            "bit_width": 5,
            "error": 21.630603790283203
        },
        "model.layers.13.self_attn.v_proj": {
            "bit_width": 5,
            "error": 25.578495025634766
        },
        "model.layers.13.self_attn.o_proj": {
            "bit_width": 5,
            "error": 25.197811126708984
        },
        "model.layers.13.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.701894760131836
        },
        "model.layers.13.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.261754989624023
        },
        "model.layers.13.mlp.down_proj": {
            "bit_width": 5,
            "error": 23.439373016357422
        },
        "model.layers.14.self_attn.q_proj": {
            "bit_width": 5,
            "error": 23.500350952148438
        },
        "model.layers.14.self_attn.k_proj": {
            "bit_width": 5,
            "error": 24.98419952392578
        },
        "model.layers.14.self_attn.v_proj": {
            "bit_width": 5,
            "error": 24.14889144897461
        },
        "model.layers.14.self_attn.o_proj": {
            "bit_width": 5,
            "error": 23.664134979248047
        },
        "model.layers.14.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.98908233642578
        },
        "model.layers.14.mlp.up_proj": {
            "bit_width": 5,
            "error": 20.13252067565918
        },
        "model.layers.14.mlp.down_proj": {
            "bit_width": 5,
            "error": 23.079784393310547
        },
        "model.layers.15.self_attn.q_proj": {
            "bit_width": 5,
            "error": 22.426403045654297
        },
        "model.layers.15.self_attn.k_proj": {
            "bit_width": 6,
            "error": 25.942895889282227
        },
        "model.layers.15.self_attn.v_proj": {
            "bit_width": 5,
            "error": 26.009151458740234
        },
        "model.layers.15.self_attn.o_proj": {
            "bit_width": 5,
            "error": 21.9721622467041
        },
        "model.layers.15.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.031023025512695
        },
        "model.layers.15.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.292682647705078
        },
        "model.layers.15.mlp.down_proj": {
            "bit_width": 5,
            "error": 23.285415649414062
        },
        "model.layers.16.self_attn.q_proj": {
            "bit_width": 5,
            "error": 20.225727081298828
        },
        "model.layers.16.self_attn.k_proj": {
            "bit_width": 6,
            "error": 25.640789031982422
        },
        "model.layers.16.self_attn.v_proj": {
            "bit_width": 5,
            "error": 25.410167694091797
        },
        "model.layers.16.self_attn.o_proj": {
            "bit_width": 5,
            "error": 24.910446166992188
        },
        "model.layers.16.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.144014358520508
        },
        "model.layers.16.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.339725494384766
        },
        "model.layers.16.mlp.down_proj": {
            "bit_width": 5,
            "error": 23.040931701660156
        },
        "model.layers.17.self_attn.q_proj": {
            "bit_width": 5,
            "error": 20.05355453491211
        },
        "model.layers.17.self_attn.k_proj": {
            "bit_width": 5,
            "error": 20.818937301635742
        },
        "model.layers.17.self_attn.v_proj": {
            "bit_width": 5,
            "error": 25.66253662109375
        },
        "model.layers.17.self_attn.o_proj": {
            "bit_width": 5,
            "error": 22.561737060546875
        },
        "model.layers.17.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.23506736755371
        },
        "model.layers.17.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.259885787963867
        },
        "model.layers.17.mlp.down_proj": {
            "bit_width": 5,
            "error": 23.447738647460938
        },
        "model.layers.18.self_attn.q_proj": {
            "bit_width": 5,
            "error": 20.026714324951172
        },
        "model.layers.18.self_attn.k_proj": {
            "bit_width": 5,
            "error": 20.72454833984375
        },
        "model.layers.18.self_attn.v_proj": {
            "bit_width": 5,
            "error": 23.8741512298584
        },
        "model.layers.18.self_attn.o_proj": {
            "bit_width": 5,
            "error": 24.067487716674805
        },
        "model.layers.18.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.37920570373535
        },
        "model.layers.18.mlp.up_proj": {
            "bit_width": 5,
            "error": 21.886396408081055
        },
        "model.layers.18.mlp.down_proj": {
            "bit_width": 5,
            "error": 23.459787368774414
        },
        "model.layers.19.self_attn.q_proj": {
            "bit_width": 5,
            "error": 20.73967742919922
        },
        "model.layers.19.self_attn.k_proj": {
            "bit_width": 5,
            "error": 21.090007781982422
        },
        "model.layers.19.self_attn.v_proj": {
            "bit_width": 5,
            "error": 23.630552291870117
        },
        "model.layers.19.self_attn.o_proj": {
            "bit_width": 5,
            "error": 23.110057830810547
        },
        "model.layers.19.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.34628677368164
        },
        "model.layers.19.mlp.up_proj": {
            "bit_width": 5,
            "error": 23.151643753051758
        },
        "model.layers.19.mlp.down_proj": {
            "bit_width": 5,
            "error": 23.59492301940918
        },
        "model.layers.20.self_attn.q_proj": {
            "bit_width": 5,
            "error": 20.74991226196289
        },
        "model.layers.20.self_attn.k_proj": {
            "bit_width": 5,
            "error": 21.565568923950195
        },
        "model.layers.20.self_attn.v_proj": {
            "bit_width": 5,
            "error": 24.47714614868164
        },
        "model.layers.20.self_attn.o_proj": {
            "bit_width": 5,
            "error": 23.157310485839844
        },
        "model.layers.20.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.437170028686523
        },
        "model.layers.20.mlp.up_proj": {
            "bit_width": 5,
            "error": 23.19371795654297
        },
        "model.layers.20.mlp.down_proj": {
            "bit_width": 5,
            "error": 23.758161544799805
        },
        "model.layers.21.self_attn.q_proj": {
            "bit_width": 5,
            "error": 23.135297775268555
        },
        "model.layers.21.self_attn.k_proj": {
            "bit_width": 5,
            "error": 22.483633041381836
        },
        "model.layers.21.self_attn.v_proj": {
            "bit_width": 5,
            "error": 24.791582107543945
        },
        "model.layers.21.self_attn.o_proj": {
            "bit_width": 5,
            "error": 23.201114654541016
        },
        "model.layers.21.mlp.gate_proj": {
            "bit_width": 5,
            "error": 21.884952545166016
        },
        "model.layers.21.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.76485824584961
        },
        "model.layers.21.mlp.down_proj": {
            "bit_width": 5,
            "error": 22.972248077392578
        },
        "model.layers.22.self_attn.q_proj": {
            "bit_width": 5,
            "error": 22.391666412353516
        },
        "model.layers.22.self_attn.k_proj": {
            "bit_width": 5,
            "error": 23.911516189575195
        },
        "model.layers.22.self_attn.v_proj": {
            "bit_width": 5,
            "error": 24.384737014770508
        },
        "model.layers.22.self_attn.o_proj": {
            "bit_width": 5,
            "error": 24.104814529418945
        },
        "model.layers.22.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.253305435180664
        },
        "model.layers.22.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.836040496826172
        },
        "model.layers.22.mlp.down_proj": {
            "bit_width": 5,
            "error": 22.799253463745117
        },
        "model.layers.23.self_attn.q_proj": {
            "bit_width": 5,
            "error": 23.21382713317871
        },
        "model.layers.23.self_attn.k_proj": {
            "bit_width": 5,
            "error": 24.874309539794922
        },
        "model.layers.23.self_attn.v_proj": {
            "bit_width": 5,
            "error": 23.928810119628906
        },
        "model.layers.23.self_attn.o_proj": {
            "bit_width": 5,
            "error": 26.20051383972168
        },
        "model.layers.23.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.34625816345215
        },
        "model.layers.23.mlp.up_proj": {
            "bit_width": 5,
            "error": 23.210927963256836
        },
        "model.layers.23.mlp.down_proj": {
            "bit_width": 5,
            "error": 23.32282829284668
        },
        "model.layers.24.self_attn.q_proj": {
            "bit_width": 5,
            "error": 23.45207977294922
        },
        "model.layers.24.self_attn.k_proj": {
            "bit_width": 5,
            "error": 23.13626480102539
        },
        "model.layers.24.self_attn.v_proj": {
            "bit_width": 5,
            "error": 24.433116912841797
        },
        "model.layers.24.self_attn.o_proj": {
            "bit_width": 5,
            "error": 24.000547409057617
        },
        "model.layers.24.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.097915649414062
        },
        "model.layers.24.mlp.up_proj": {
            "bit_width": 5,
            "error": 22.43032455444336
        },
        "model.layers.24.mlp.down_proj": {
            "bit_width": 5,
            "error": 24.194595336914062
        },
        "model.layers.25.self_attn.q_proj": {
            "bit_width": 5,
            "error": 23.31611442565918
        },
        "model.layers.25.self_attn.k_proj": {
            "bit_width": 5,
            "error": 24.657602310180664
        },
        "model.layers.25.self_attn.v_proj": {
            "bit_width": 5,
            "error": 24.520328521728516
        },
        "model.layers.25.self_attn.o_proj": {
            "bit_width": 5,
            "error": 26.238317489624023
        },
        "model.layers.25.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.398029327392578
        },
        "model.layers.25.mlp.up_proj": {
            "bit_width": 5,
            "error": 23.65325164794922
        },
        "model.layers.25.mlp.down_proj": {
            "bit_width": 5,
            "error": 23.946979522705078
        },
        "model.layers.26.self_attn.q_proj": {
            "bit_width": 5,
            "error": 23.620452880859375
        },
        "model.layers.26.self_attn.k_proj": {
            "bit_width": 5,
            "error": 24.077377319335938
        },
        "model.layers.26.self_attn.v_proj": {
            "bit_width": 5,
            "error": 24.84427261352539
        },
        "model.layers.26.self_attn.o_proj": {
            "bit_width": 4,
            "error": 21.2430477142334
        },
        "model.layers.26.mlp.gate_proj": {
            "bit_width": 5,
            "error": 21.72087860107422
        },
        "model.layers.26.mlp.up_proj": {
            "bit_width": 5,
            "error": 23.30150604248047
        },
        "model.layers.26.mlp.down_proj": {
            "bit_width": 6,
            "error": 25.992353439331055
        },
        "model.layers.27.self_attn.q_proj": {
            "bit_width": 5,
            "error": 23.414710998535156
        },
        "model.layers.27.self_attn.k_proj": {
            "bit_width": 5,
            "error": 21.85692024230957
        },
        "model.layers.27.self_attn.v_proj": {
            "bit_width": 5,
            "error": 24.667339324951172
        },
        "model.layers.27.self_attn.o_proj": {
            "bit_width": 5,
            "error": 26.20074462890625
        },
        "model.layers.27.mlp.gate_proj": {
            "bit_width": 5,
            "error": 22.761125564575195
        },
        "model.layers.27.mlp.up_proj": {
            "bit_width": 5,
            "error": 23.23343276977539
        },
        "model.layers.27.mlp.down_proj": {
            "bit_width": 5,
            "error": 23.98511505126953
        },
        "model.layers.28.self_attn.q_proj": {
            "bit_width": 5,
            "error": 22.14319610595703
        },
        "model.layers.28.self_attn.k_proj": {
            "bit_width": 5,
            "error": 21.913349151611328
        },
        "model.layers.28.self_attn.v_proj": {
            "bit_width": 5,
            "error": 24.02914047241211
        },
        "model.layers.28.self_attn.o_proj": {
            "bit_width": 5,
            "error": 25.367847442626953
        },
        "model.layers.28.mlp.gate_proj": {
            "bit_width": 6,
            "error": 21.213024139404297
        },
        "model.layers.28.mlp.up_proj": {
            "bit_width": 6,
            "error": 23.584335327148438
        },
        "model.layers.28.mlp.down_proj": {
            "bit_width": 6,
            "error": 21.869651794433594
        },
        "model.layers.29.self_attn.q_proj": {
            "bit_width": 5,
            "error": 22.07087516784668
        },
        "model.layers.29.self_attn.k_proj": {
            "bit_width": 5,
            "error": 24.048812866210938
        },
        "model.layers.29.self_attn.v_proj": {
            "bit_width": 5,
            "error": 23.531700134277344
        },
        "model.layers.29.self_attn.o_proj": {
            "bit_width": 5,
            "error": 24.694759368896484
        },
        "model.layers.29.mlp.gate_proj": {
            "bit_width": 5,
            "error": 21.99517059326172
        },
        "model.layers.29.mlp.up_proj": {
            "bit_width": 6,
            "error": 23.278423309326172
        },
        "model.layers.29.mlp.down_proj": {
            "bit_width": 6,
            "error": 25.002901077270508
        },
        "lm_head": {
            "bit_width": 5,
            "error": 25.10092544555664
        }
    },
    "average_bit_width": 5.071090047393365,
    "error_threshold": 20,
    "min_quantile": 0,
    "max_quantile": 1,
    "quantized_model_accuracy": 0.3808930795232165
}